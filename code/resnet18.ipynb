{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用ResNet18在CIFAR10上训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理数据集\n",
    "这里先处理data_batch_1中的一万个数据\n",
    "按照官网中的说明使用unpickle函数进行读取,然后使用TensorDataset进行封装。\n",
    "另外这里有一个细节，由于数据集中我们的训练数据是五万张图像，如果读取出来的五万张图像的List直接转为张量，那么速度会很慢，我们先使用numpy.array()将ndarray的list转化为single ndarray，转为张量的过程就会显著加速。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'ianafp/CIFAR10/cifar-10-batches-py/data_batch_'\n",
    "TEST_BATCH = 'ianafp/CIFAR10/cifar-10-batches-py/test_batch'\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "x_train,y_train = [],[]\n",
    "for i in range(1,6):\n",
    "    dataset = unpickle(PATH+'{}'.format(i))\n",
    "    # print(dataset.keys())\n",
    "    x_train.extend(dataset[b'data'])\n",
    "    y_train.extend(dataset[b'labels'])\n",
    "dataset = unpickle(TEST_BATCH)\n",
    "x_valid,y_valid = dataset[b'data'],dataset[b'labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试数据有效性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.__len__())\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "img = np.reshape(x_valid[8000],(3,32,32))\n",
    "img = np.transpose(img,(1,2,0))\n",
    "pyplot.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据使用dataset和dataloader类进行封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "import torch\n",
    "\n",
    "print(x_train.__len__())\n",
    "batch_size = 256 \n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_valid = np.array(x_valid)\n",
    "y_valid = np.array(y_valid)\n",
    "x_train = np.reshape(x_train,(x_train.__len__(),3,32,32))\n",
    "x_valid = np.reshape(x_valid,(x_valid.__len__(),3,32,32))\n",
    "train_ds = TensorDataset(torch.tensor(x_train,dtype=torch.float),torch.tensor(y_train,dtype=torch.float))\n",
    "train_dl = DataLoader(train_ds,batch_size=batch_size)\n",
    "valid_ds = TensorDataset(torch.tensor(x_valid,dtype=torch.float),torch.tensor(y_valid,dtype=torch.float))\n",
    "valid_dl = DataLoader(valid_ds,batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用torchvison中的ResNet18\n",
    "这里我们使用torchvision中的ResNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18,ResNet18_Weights\n",
    "model = resnet18()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置损失函数，这里我们使用交叉熵损失函数(croos entropy error)\n",
    "$$\n",
    "E = - \\sum_k t_k \\log y_k\n",
    "$$\n",
    "其中，$t_k$为label,$y_k$为网络输出值\n",
    "这里实现使用torch.nn.functional中封装好的交叉熵函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现一个准确率函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练前检验我们模型的损失函数值和准确率。\n",
    "这里面要注意的点是pytorch卷积层是以浮点数工作的，而我们从图像中读取的训练数据集是byte类型的，因而我们要对tensor作类型转化。\n",
    "另外torch.nn.functional中的交叉熵损失函数F.cross_entropy中标签以long类型工作，因而也需要作类型转化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = train_ds[0:batch_size]\n",
    "pred = model(x)\n",
    "print(pred.dtype,y.dtype)\n",
    "print('loss_func = ',loss_func(pred,y.long()))\n",
    "print('accuracy = ',accuracy(pred,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用torch.optim进行梯度下降优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "learning_rate = 0.5\n",
    "opt = optim.SGD(model.parameters(),lr=learning_rate)\n",
    "epochs = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义随机梯度下降函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model,loss_func,xb,yb,opt=None):\n",
    "    loss = loss_func(model(xb),yb.long())\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    return loss.item(),len(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义fit函数，fit函数中完成训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fit(epochs,model,loss_func,opt,train_dl,valid_dl):\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         for xb,yb in train_dl:\n",
    "#             loss_batch(model,loss_func,xb,yb.long(),opt)\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             losses,nums = zip(*[loss_batch(model,loss_func,xb,yb) for xb,yb in valid_dl])\n",
    "#         val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "#         print(epoch,val_loss)\n",
    "\n",
    "def fit(epochs,model,loss_func,opt,train_dl,valid_dl):\n",
    "    epoch = 0\n",
    "    pre_loss = 0\n",
    "    while True:\n",
    "        model.train()\n",
    "        for xb,yb in train_dl:\n",
    "            loss_batch(model,loss_func,xb,yb.long(),opt)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses,nums = zip(*[loss_batch(model,loss_func,xb,yb) for xb,yb in valid_dl])\n",
    "        val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "        print(epoch,val_loss)\n",
    "        import math \n",
    "        if math.fabs(val_loss-pre_loss)<1e-9:\n",
    "            break\n",
    "        pre_loss = val_loss\n",
    "        epoch = epoch + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "试运行训练函数fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit(epochs,model,loss_func,opt,train_dl,valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行时间过长。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将模型部署在GPU上\n",
    "检验cuda是否可用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据集迁移到GPU\n",
    "这里使用的技巧是定义了WrappedDataLoader类，该类是可迭代的，因为定义了__iter__方法。\n",
    "又因为__iter__方法中使用了关键字yield，因而该类可做生成器(generater)。\n",
    "在后续\n",
    "```python\n",
    "    for x,y in train_dl:\n",
    "        ...\n",
    "```\n",
    "的过程中会动态的将数据应用preprocess方法返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x,y):\n",
    "    return x.to(dev),y.to(dev)\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            yield (self.func(*b))\n",
    "train_dl = WrappedDataLoader(train_dl,preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl,preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将模型迁移到GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(dev)\n",
    "opt = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "试运行训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检验准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = valid_ds[0:batch_size]\n",
    "x = x.to(dev)\n",
    "y = y.to(dev)\n",
    "pred = model(x)\n",
    "print(pred.dtype,y.dtype)\n",
    "print('loss_func = ',loss_func(pred,y.long()))\n",
    "print('accuracy = ',accuracy(pred,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行结果表明，固定训练轮次准确率较低，若以$1\\times 10^{-9}$为损失函数的收敛阈值，则运行无法收敛，具体原因暂未得知。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ianafp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
