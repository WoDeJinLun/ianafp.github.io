<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2023/07/08/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very
first post. Check <a href="https://hexo.io/docs/">documentation</a> for
more info. If you get any problems when using Hexo, you can find the
answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or
you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
<video src="/vid.mp4">
my video
</video>
]]></content>
      <tags>
        <tag>博客使用说明</tag>
      </tags>
  </entry>
  <entry>
    <title>nn_tutorial</title>
    <url>/2023/07/13/nn-tutorial/</url>
    <content><![CDATA[<p>这篇文章是官网上的一篇jupyter notebook,
主要讲了pytorch学习的工作原理，以及如何一步步从手动训练模型到使用pytorch高度抽象化地训练模型。
<span id="more"></span> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># For tips on running notebooks in Google Colab, see</span></span><br><span class="line"><span class="comment"># https://pytorch.org/tutorials/beginner/colab</span></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure></p>
<h1 id="what-is-torch.nn-really">What is <code>torch.nn</code>
<em>really</em>?</h1>
<p><strong>Authors:</strong> Jeremy Howard, <a href="https://www.fast.ai">fast.ai</a>. Thanks to Rachel Thomas and
Francisco Ingham.</p>
<p>We recommend running this tutorial as a notebook, not a script. To
download the notebook (<code>.ipynb</code>) file, click the link at the
top of the page.</p>
<p>PyTorch provides the elegantly designed modules and classes <a href="https://pytorch.org/docs/stable/nn.html">torch.nn</a> , <a href="https://pytorch.org/docs/stable/optim.html">torch.optim</a> , <a href="https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset">Dataset</a>
, and <a href="https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader">DataLoader</a>
to help you create and train neural networks. In order to fully utilize
their power and customize them for your problem, you need to really
understand exactly what they're doing. To develop this understanding, we
will first train basic neural net on the MNIST data set without using
any features from these models; we will initially only use the most
basic PyTorch tensor functionality. Then, we will incrementally add one
feature from <code>torch.nn</code>, <code>torch.optim</code>,
<code>Dataset</code>, or <code>DataLoader</code> at a time, showing
exactly what each piece does, and how it works to make the code either
more concise, or more flexible.</p>
<p><strong>This tutorial assumes you already have PyTorch installed, and
are familiar with the basics of tensor operations.</strong> (If you're
familiar with Numpy array operations, you'll find the PyTorch tensor
operations used here nearly identical).</p>
<h2 id="mnist-data-setup">MNIST data setup</h2>
<p>We will use the classic <a href="http://deeplearning.net/data/mnist/">MNIST</a> dataset, which
consists of black-and-white images of hand-drawn digits (between 0 and
9).</p>
<p>We will use <a href="https://docs.python.org/3/library/pathlib.html">pathlib</a> for
dealing with paths (part of the Python 3 standard library), and will
download the dataset using <a href="http://docs.python-requests.org/en/master/">requests</a>. We will
only import modules when we use them, so you can see exactly what's
being used at each point.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">DATA_PATH = Path(<span class="string">&quot;data&quot;</span>)</span><br><span class="line">PATH = DATA_PATH / <span class="string">&quot;mnist&quot;</span></span><br><span class="line"></span><br><span class="line">PATH.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">URL = <span class="string">&quot;https://github.com/pytorch/tutorials/raw/main/_static/&quot;</span></span><br><span class="line">FILENAME = <span class="string">&quot;mnist.pkl.gz&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> (PATH / FILENAME).exists():</span><br><span class="line">        content = requests.get(URL + FILENAME).content</span><br><span class="line">        (PATH / FILENAME).<span class="built_in">open</span>(<span class="string">&quot;wb&quot;</span>).write(content)</span><br></pre></td></tr></table></figure>
<p>This dataset is in numpy array format, and has been stored using
pickle, a python-specific format for serializing data.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> gzip.<span class="built_in">open</span>((PATH / FILENAME).as_posix(), <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=<span class="string">&quot;latin-1&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>Each image is 28 x 28, and is being stored as a flattened row of
length 784 (=28x28). Let's take a look at one; we need to reshape it to
2d first.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">pyplot.imshow(x_train[<span class="number">0</span>].reshape((<span class="number">28</span>, <span class="number">28</span>)), cmap=<span class="string">&quot;gray&quot;</span>)</span><br><span class="line"><span class="comment"># ``pyplot.show()`` only if not on Colab</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">import</span> google.colab</span><br><span class="line"><span class="keyword">except</span> ImportError:</span><br><span class="line">    pyplot.show()</span><br><span class="line"><span class="built_in">print</span>(x_train.shape)</span><br></pre></td></tr></table></figure>
<p>PyTorch uses <code>torch.tensor</code>, rather than numpy arrays, so
we need to convert our data.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x_train, y_train, x_valid, y_valid = <span class="built_in">map</span>(</span><br><span class="line">    torch.tensor, (x_train, y_train, x_valid, y_valid)</span><br><span class="line">)</span><br><span class="line">n, c = x_train.shape</span><br><span class="line"><span class="built_in">print</span>(x_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(x_train.shape)</span><br><span class="line"><span class="built_in">print</span>(y_train.<span class="built_in">min</span>(), y_train.<span class="built_in">max</span>())</span><br></pre></td></tr></table></figure>
<h2 id="neural-net-from-scratch-without-torch.nn">Neural net from
scratch (without <code>torch.nn</code>)</h2>
<p>Let's first create a model using nothing but PyTorch tensor
operations. We're assuming you're already familiar with the basics of
neural networks. (If you're not, you can learn them at <a href="https://course.fast.ai">course.fast.ai</a>).</p>
<p>PyTorch provides methods to create random or zero-filled tensors,
which we will use to create our weights and bias for a simple linear
model. These are just regular tensors, with one very special addition:
we tell PyTorch that they require a gradient. This causes PyTorch to
record all of the operations done on the tensor, so that it can
calculate the gradient during back-propagation
<em>automatically</em>!</p>
<p>For the weights, we set <code>requires_grad</code>
<strong>after</strong> the initialization, since we don't want that step
included in the gradient. (Note that a trailing <code>_</code> in
PyTorch signifies that the operation is performed in-place.)</p>
<div class="alert alert-info">
<h4>
Note
</h4>
<p>
We are initializing the weights here with <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Xavier
initialisation</a> (by multiplying with <code>1/sqrt(n)</code>).
</p>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">weights = torch.randn(<span class="number">784</span>, <span class="number">10</span>) / math.sqrt(<span class="number">784</span>)</span><br><span class="line">weights.requires_grad_()</span><br><span class="line">bias = torch.zeros(<span class="number">10</span>, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>Thanks to PyTorch's ability to calculate gradients automatically, we
can use any standard Python function (or callable object) as a model! So
let's just write a plain matrix multiplication and broadcasted addition
to create a simple linear model. We also need an activation function, so
we'll write <code>log_softmax</code> and use it. Remember: although
PyTorch provides lots of prewritten loss functions, activation
functions, and so forth, you can easily write your own using plain
python. PyTorch will even create fast GPU or vectorized CPU code for
your function automatically.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">log_softmax</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x - x.exp().<span class="built_in">sum</span>(-<span class="number">1</span>).log().unsqueeze(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model</span>(<span class="params">xb</span>):</span><br><span class="line">    <span class="keyword">return</span> log_softmax(xb @ weights + bias)</span><br></pre></td></tr></table></figure>
<p>In the above, the <code>@</code> stands for the matrix multiplication
operation. We will call our function on one batch of data (in this case,
64 images). This is one <em>forward pass</em>. Note that our predictions
won't be any better than random at this stage, since we start with
random weights.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bs = <span class="number">64</span>  <span class="comment"># batch size</span></span><br><span class="line"></span><br><span class="line">xb = x_train[<span class="number">0</span>:bs]  <span class="comment"># a mini-batch from x</span></span><br><span class="line">preds = model(xb)  <span class="comment"># predictions</span></span><br><span class="line">preds[<span class="number">0</span>], preds.shape</span><br><span class="line"><span class="built_in">print</span>(preds[<span class="number">0</span>], preds.shape)</span><br></pre></td></tr></table></figure>
<p>As you see, the <code>preds</code> tensor contains not only the
tensor values, but also a gradient function. We'll use this later to do
backprop.</p>
<p>Let's implement negative log-likelihood to use as the loss function
(again, we can just use standard Python):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">nll</span>(<span class="params"><span class="built_in">input</span>, target</span>):</span><br><span class="line">    <span class="keyword">return</span> -<span class="built_in">input</span>[<span class="built_in">range</span>(target.shape[<span class="number">0</span>]), target].mean()</span><br><span class="line"></span><br><span class="line">loss_func = nll</span><br></pre></td></tr></table></figure>
<p>Let's check our loss with our random model, so we can see if we
improve after a backprop pass later.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">yb = y_train[<span class="number">0</span>:bs]</span><br><span class="line"><span class="built_in">print</span>(loss_func(preds, yb))</span><br></pre></td></tr></table></figure>
<p>Let's also implement a function to calculate the accuracy of our
model. For each prediction, if the index with the largest value matches
the target value, then the prediction was correct.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">out, yb</span>):</span><br><span class="line">    preds = torch.argmax(out, dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (preds == yb).<span class="built_in">float</span>().mean()</span><br></pre></td></tr></table></figure>
<p>Let's check the accuracy of our random model, so we can see if our
accuracy improves as our loss improves.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(accuracy(preds, yb))</span><br></pre></td></tr></table></figure>
<p>We can now run a training loop. For each iteration, we will:</p>
<ul>
<li>select a mini-batch of data (of size <code>bs</code>)</li>
<li>use the model to make predictions</li>
<li>calculate the loss</li>
<li><code>loss.backward()</code> updates the gradients of the model, in
this case, <code>weights</code> and <code>bias</code>.</li>
</ul>
<p>We now use these gradients to update the weights and bias. We do this
within the <code>torch.no_grad()</code> context manager, because we do
not want these actions to be recorded for our next calculation of the
gradient. You can read more about how PyTorch's Autograd records
operations <a href="https://pytorch.org/docs/stable/notes/autograd.html">here</a>.</p>
<p>We then set the gradients to zero, so that we are ready for the next
loop. Otherwise, our gradients would record a running tally of all the
operations that had happened (i.e. <code>loss.backward()</code>
<em>adds</em> the gradients to whatever is already stored, rather than
replacing them).</p>
<p>.. tip:: You can use the standard python debugger to step through
PyTorch code, allowing you to check the various variable values at each
step. Uncomment <code>set_trace()</code> below to try it out.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython.core.debugger <span class="keyword">import</span> set_trace</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.5</span>  <span class="comment"># learning rate</span></span><br><span class="line">epochs = <span class="number">2</span>  <span class="comment"># how many epochs to train for</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">        <span class="comment">#         set_trace()</span></span><br><span class="line">        start_i = i * bs</span><br><span class="line">        end_i = start_i + bs</span><br><span class="line">        xb = x_train[start_i:end_i]</span><br><span class="line">        yb = y_train[start_i:end_i]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            weights -= weights.grad * lr</span><br><span class="line">            bias -= bias.grad * lr</span><br><span class="line">            weights.grad.zero_()</span><br><span class="line">            bias.grad.zero_()</span><br></pre></td></tr></table></figure>
<p>That's it: we've created and trained a minimal neural network (in
this case, a logistic regression, since we have no hidden layers)
entirely from scratch!</p>
<p>Let's check the loss and accuracy and compare those to what we got
earlier. We expect that the loss will have decreased and accuracy to
have increased, and they have.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb), accuracy(model(xb), yb))</span><br></pre></td></tr></table></figure>
<h2 id="using-torch.nn.functional">Using
<code>torch.nn.functional</code></h2>
<p>We will now refactor our code, so that it does the same thing as
before, only we'll start taking advantage of PyTorch's <code>nn</code>
classes to make it more concise and flexible. At each step from here, we
should be making our code one or more of: shorter, more understandable,
and/or more flexible.</p>
<p>The first and easiest step is to make our code shorter by replacing
our hand-written activation and loss functions with those from
<code>torch.nn.functional</code> (which is generally imported into the
namespace <code>F</code> by convention). This module contains all the
functions in the <code>torch.nn</code> library (whereas other parts of
the library contain classes). As well as a wide range of loss and
activation functions, you'll also find here some convenient functions
for creating neural nets, such as pooling functions. (There are also
functions for doing convolutions, linear layers, etc, but as we'll see,
these are usually better handled using other parts of the library.)</p>
<p>If you're using negative log likelihood loss and log softmax
activation, then Pytorch provides a single function
<code>F.cross_entropy</code> that combines the two. So we can even
remove the activation function from our model.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">loss_func = F.cross_entropy</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model</span>(<span class="params">xb</span>):</span><br><span class="line">    <span class="keyword">return</span> xb @ weights + bias</span><br></pre></td></tr></table></figure>
<p>Note that we no longer call <code>log_softmax</code> in the
<code>model</code> function. Let's confirm that our loss and accuracy
are the same as before:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb), accuracy(model(xb), yb))</span><br></pre></td></tr></table></figure>
<h2 id="refactor-using-nn.module">Refactor using
<code>nn.Module</code></h2>
<p>Next up, we'll use <code>nn.Module</code> and
<code>nn.Parameter</code>, for a clearer and more concise training loop.
We subclass <code>nn.Module</code> (which itself is a class and able to
keep track of state). In this case, we want to create a class that holds
our weights, bias, and method for the forward step.
<code>nn.Module</code> has a number of attributes and methods (such as
<code>.parameters()</code> and <code>.zero_grad()</code>) which we will
be using.</p>
<div class="alert alert-info">
<h4>
Note
</h4>
<p>
<code>nn.Module</code> (uppercase M) is a PyTorch specific concept, and
is a class we'll be using a lot. <code>nn.Module</code> is not to be
confused with the Python concept of a (lowercase <code>m</code>) <a href="https://docs.python.org/3/tutorial/modules.html">module</a>, which
is a file of Python code that can be imported.
</p>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Mnist_Logistic</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weights = nn.Parameter(torch.randn(<span class="number">784</span>, <span class="number">10</span>) / math.sqrt(<span class="number">784</span>))</span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, xb</span>):</span><br><span class="line">        <span class="keyword">return</span> xb @ self.weights + self.bias</span><br></pre></td></tr></table></figure>
<p>Since we're now using an object instead of just using a function, we
first have to instantiate our model:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Mnist_Logistic()</span><br></pre></td></tr></table></figure>
<p>Now we can calculate the loss in the same way as before. Note that
<code>nn.Module</code> objects are used as if they are functions (i.e
they are <em>callable</em>), but behind the scenes Pytorch will call our
<code>forward</code> method automatically.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<p>Previously for our training loop we had to update the values for each
parameter by name, and manually zero out the grads for each parameter
separately, like this:</p>
<p>::</p>
<p>with torch.no_grad(): weights -= weights.grad * lr bias -= bias.grad
* lr weights.grad.zero_() bias.grad.zero_()</p>
<p>Now we can take advantage of model.parameters() and model.zero_grad()
(which are both defined by PyTorch for <code>nn.Module</code>) to make
those steps more concise and less prone to the error of forgetting some
of our parameters, particularly if we had a more complicated model:</p>
<p>::</p>
<p>with torch.no_grad(): for p in model.parameters(): p -= p.grad * lr
model.zero_grad()</p>
<p>We'll wrap our little training loop in a <code>fit</code> function so
we can run it again later.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>():</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">            start_i = i * bs</span><br><span class="line">            end_i = start_i + bs</span><br><span class="line">            xb = x_train[start_i:end_i]</span><br><span class="line">            yb = y_train[start_i:end_i]</span><br><span class="line">            pred = model(xb)</span><br><span class="line">            loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">                    p -= p.grad * lr</span><br><span class="line">                model.zero_grad()</span><br><span class="line"></span><br><span class="line">fit()</span><br></pre></td></tr></table></figure>
<p>Let's double-check that our loss has gone down:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<h2 id="refactor-using-nn.linear">Refactor using
<code>nn.Linear</code></h2>
<p>We continue to refactor our code. Instead of manually defining and
initializing <code>self.weights</code> and <code>self.bias</code>, and
calculating <code>xb  @ self.weights + self.bias</code>, we will instead
use the Pytorch class <a href="https://pytorch.org/docs/stable/nn.html#linear-layers">nn.Linear</a>
for a linear layer, which does all that for us. Pytorch has many types
of predefined layers that can greatly simplify our code, and often makes
it faster too.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Mnist_Logistic</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.lin = nn.Linear(<span class="number">784</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, xb</span>):</span><br><span class="line">        <span class="keyword">return</span> self.lin(xb)</span><br></pre></td></tr></table></figure>
<p>We instantiate our model and calculate the loss in the same way as
before:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Mnist_Logistic()</span><br><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<p>We are still able to use our same <code>fit</code> method as
before.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<h2 id="refactor-using-torch.optim">Refactor using
<code>torch.optim</code></h2>
<p>Pytorch also has a package with various optimization algorithms,
<code>torch.optim</code>. We can use the <code>step</code> method from
our optimizer to take a forward step, instead of manually updating each
parameter.</p>
<p>This will let us replace our previous manually coded optimization
step:</p>
<p>::</p>
<p>with torch.no_grad(): for p in model.parameters(): p -= p.grad * lr
model.zero_grad()</p>
<p>and instead use just:</p>
<p>::</p>
<p>opt.step() opt.zero_grad()</p>
<p>(<code>optim.zero_grad()</code> resets the gradient to 0 and we need
to call it before computing the gradient for the next minibatch.)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br></pre></td></tr></table></figure>
<p>We'll define a little function to create our model and optimizer so
we can reuse it in the future.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_model</span>():</span><br><span class="line">    model = Mnist_Logistic()</span><br><span class="line">    <span class="keyword">return</span> model, optim.SGD(model.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">model, opt = get_model()</span><br><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">        start_i = i * bs</span><br><span class="line">        end_i = start_i + bs</span><br><span class="line">        xb = x_train[start_i:end_i]</span><br><span class="line">        yb = y_train[start_i:end_i]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<h2 id="refactor-using-dataset">Refactor using Dataset</h2>
<p>PyTorch has an abstract Dataset class. A Dataset can be anything that
has a <code>__len__</code> function (called by Python's standard
<code>len</code> function) and a <code>__getitem__</code> function as a
way of indexing into it. <a href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html">This
tutorial</a> walks through a nice example of creating a custom
<code>FacialLandmarkDataset</code> class as a subclass of
<code>Dataset</code>.</p>
<p>PyTorch's <a href="https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset">TensorDataset</a>
is a Dataset wrapping tensors. By defining a length and way of indexing,
this also gives us a way to iterate, index, and slice along the first
dimension of a tensor. This will make it easier to access both the
independent and dependent variables in the same line as we train.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset</span><br></pre></td></tr></table></figure>
<p>Both <code>x_train</code> and <code>y_train</code> can be combined in
a single <code>TensorDataset</code>, which will be easier to iterate
over and slice.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br></pre></td></tr></table></figure>
<p>Previously, we had to iterate through minibatches of <code>x</code>
and <code>y</code> values separately:</p>
<p>::</p>
<p>xb = x_train[start_i:end_i] yb = y_train[start_i:end_i]</p>
<p>Now, we can do these two steps together:</p>
<p>::</p>
<p>xb,yb = train_ds[i<em>bs : i</em>bs+bs]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">        xb, yb = train_ds[i * bs: i * bs + bs]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<h2 id="refactor-using-dataloader">Refactor using
<code>DataLoader</code></h2>
<p>PyTorch's <code>DataLoader</code> is responsible for managing
batches. You can create a <code>DataLoader</code> from any
<code>Dataset</code>. <code>DataLoader</code> makes it easier to iterate
over batches. Rather than having to use
<code>train_ds[i*bs : i*bs+bs]</code>, the <code>DataLoader</code> gives
us each minibatch automatically.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br><span class="line">train_dl = DataLoader(train_ds, batch_size=bs)</span><br></pre></td></tr></table></figure>
<p>Previously, our loop iterated over batches <code>(xb, yb)</code> like
this:</p>
<p>::</p>
<p>for i in range((n-1)//bs + 1): xb,yb = train_ds[i<em>bs :
i</em>bs+bs] pred = model(xb)</p>
<p>Now, our loop is much cleaner, as <code>(xb, yb)</code> are loaded
automatically from the data loader:</p>
<p>::</p>
<p>for xb,yb in train_dl: pred = model(xb)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<p>Thanks to PyTorch's <code>nn.Module</code>,
<code>nn.Parameter</code>, <code>Dataset</code>, and
<code>DataLoader</code>, our training loop is now dramatically smaller
and easier to understand. Let's now try to add the basic features
necessary to create effective models in practice.</p>
<h2 id="add-validation">Add validation</h2>
<p>In section 1, we were just trying to get a reasonable training loop
set up for use on our training data. In reality, you
<strong>always</strong> should also have a <a href="https://www.fast.ai/2017/11/13/validation-sets/">validation
set</a>, in order to identify if you are overfitting.</p>
<p>Shuffling the training data is <a href="https://www.quora.com/Does-the-order-of-training-data-matter-when-training-neural-networks">important</a>
to prevent correlation between batches and overfitting. On the other
hand, the validation loss will be identical whether we shuffle the
validation set or not. Since shuffling takes extra time, it makes no
sense to shuffle the validation data.</p>
<p>We'll use a batch size for the validation set that is twice as large
as that for the training set. This is because the validation set does
not need backpropagation and thus takes less memory (it doesn't need to
store the gradients). We take advantage of this to use a larger batch
size and compute the loss more quickly.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br><span class="line">train_dl = DataLoader(train_ds, batch_size=bs, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">valid_ds = TensorDataset(x_valid, y_valid)</span><br><span class="line">valid_dl = DataLoader(valid_ds, batch_size=bs * <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>We will calculate and print the validation loss at the end of each
epoch.</p>
<p>(Note that we always call <code>model.train()</code> before training,
and <code>model.eval()</code> before inference, because these are used
by layers such as <code>nn.BatchNorm2d</code> and
<code>nn.Dropout</code> to ensure appropriate behavior for these
different phases.)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        valid_loss = <span class="built_in">sum</span>(loss_func(model(xb), yb) <span class="keyword">for</span> xb, yb <span class="keyword">in</span> valid_dl)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(epoch, valid_loss / <span class="built_in">len</span>(valid_dl))</span><br></pre></td></tr></table></figure>
<h2 id="create-fit-and-get_data">Create fit() and get_data()</h2>
<p>We'll now do a little refactoring of our own. Since we go through a
similar process twice of calculating the loss for both the training set
and the validation set, let's make that into its own function,
<code>loss_batch</code>, which computes the loss for one batch.</p>
<p>We pass an optimizer in for the training set, and use it to perform
backprop. For the validation set, we don't pass an optimizer, so the
method doesn't perform backprop.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">loss_batch</span>(<span class="params">model, loss_func, xb, yb, opt=<span class="literal">None</span></span>):</span><br><span class="line">    loss = loss_func(model(xb), yb)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss.item(), <span class="built_in">len</span>(xb)</span><br></pre></td></tr></table></figure>
<p><code>fit</code> runs the necessary operations to train our model and
compute the training and validation losses for each epoch.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">epochs, model, loss_func, opt, train_dl, valid_dl</span>):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">            loss_batch(model, loss_func, xb, yb, opt)</span><br><span class="line"></span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            losses, nums = <span class="built_in">zip</span>(</span><br><span class="line">                *[loss_batch(model, loss_func, xb, yb) <span class="keyword">for</span> xb, yb <span class="keyword">in</span> valid_dl]</span><br><span class="line">            )</span><br><span class="line">        val_loss = np.<span class="built_in">sum</span>(np.multiply(losses, nums)) / np.<span class="built_in">sum</span>(nums)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(epoch, val_loss)</span><br></pre></td></tr></table></figure>
<p><code>get_data</code> returns dataloaders for the training and
validation sets.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_data</span>(<span class="params">train_ds, valid_ds, bs</span>):</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        DataLoader(train_ds, batch_size=bs, shuffle=<span class="literal">True</span>),</span><br><span class="line">        DataLoader(valid_ds, batch_size=bs * <span class="number">2</span>),</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>Now, our whole process of obtaining the data loaders and fitting the
model can be run in 3 lines of code:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_dl, valid_dl = get_data(train_ds, valid_ds, bs)</span><br><span class="line">model, opt = get_model()</span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>
<p>You can use these basic 3 lines of code to train a wide variety of
models. Let's see if we can use them to train a convolutional neural
network (CNN)!</p>
<h2 id="switch-to-cnn">Switch to CNN</h2>
<p>We are now going to build our neural network with three convolutional
layers. Because none of the functions in the previous section assume
anything about the model form, we'll be able to use them to train a CNN
without any modification.</p>
<p>We will use PyTorch's predefined <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d">Conv2d</a>
class as our convolutional layer. We define a CNN with 3 convolutional
layers. Each convolution is followed by a ReLU. At the end, we perform
an average pooling. (Note that <code>view</code> is PyTorch's version of
Numpy's <code>reshape</code>)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Mnist_CNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, xb</span>):</span><br><span class="line">        xb = xb.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">        xb = F.relu(self.conv1(xb))</span><br><span class="line">        xb = F.relu(self.conv2(xb))</span><br><span class="line">        xb = F.relu(self.conv3(xb))</span><br><span class="line">        xb = F.avg_pool2d(xb, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">return</span> xb.view(-<span class="number">1</span>, xb.size(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.1</span></span><br></pre></td></tr></table></figure>
<p><a href="https://cs231n.github.io/neural-networks-3/#sgd">Momentum</a> is a
variation on stochastic gradient descent that takes previous updates
into account as well and generally leads to faster training.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Mnist_CNN()</span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>
<h2 id="using-nn.sequential">Using <code>nn.Sequential</code></h2>
<p><code>torch.nn</code> has another handy class we can use to simplify
our code: <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential">Sequential</a>
. A <code>Sequential</code> object runs each of the modules contained
within it, in a sequential manner. This is a simpler way of writing our
neural network.</p>
<p>To take advantage of this, we need to be able to easily define a
<strong>custom layer</strong> from a given function. For instance,
PyTorch doesn't have a <code>view</code> layer, and we need to create
one for our network. <code>Lambda</code> will create a layer that we can
then use when defining a network with <code>Sequential</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Lambda</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, func</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.func = func</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.func(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br></pre></td></tr></table></figure>
<p>The model created with <code>Sequential</code> is simple:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">    Lambda(preprocess),</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.AvgPool2d(<span class="number">4</span>),</span><br><span class="line">    Lambda(<span class="keyword">lambda</span> x: x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>
<h2 id="wrapping-dataloader">Wrapping <code>DataLoader</code></h2>
<p>Our CNN is fairly concise, but it only works with MNIST, because: -
It assumes the input is a 28*28 long vector - It assumes that the final
CNN grid size is 4*4 (since that's the average pooling kernel size we
used)</p>
<p>Let's get rid of these two assumptions, so our model works with any
2d single channel image. First, we can remove the initial Lambda layer
by moving the data preprocessing into a generator:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span> x.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WrappedDataLoader</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dl, func</span>):</span><br><span class="line">        self.dl = dl</span><br><span class="line">        self.func = func</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.dl)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> b <span class="keyword">in</span> self.dl:</span><br><span class="line">            <span class="keyword">yield</span> (self.func(*b))</span><br><span class="line"></span><br><span class="line">train_dl, valid_dl = get_data(train_ds, valid_ds, bs)</span><br><span class="line">train_dl = WrappedDataLoader(train_dl, preprocess)</span><br><span class="line">valid_dl = WrappedDataLoader(valid_dl, preprocess)</span><br></pre></td></tr></table></figure>
<p>Next, we can replace <code>nn.AvgPool2d</code> with
<code>nn.AdaptiveAvgPool2d</code>, which allows us to define the size of
the <em>output</em> tensor we want, rather than the <em>input</em>
tensor we have. As a result, our model will work with any size
input.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.AdaptiveAvgPool2d(<span class="number">1</span>),</span><br><span class="line">    Lambda(<span class="keyword">lambda</span> x: x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<p>Let's try it out:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>
<h2 id="using-your-gpu">Using your GPU</h2>
<p>If you're lucky enough to have access to a CUDA-capable GPU (you can
rent one for about $0.50/hour from most cloud providers) you can use it
to speed up your code. First check that your GPU is working in
Pytorch:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())</span><br></pre></td></tr></table></figure>
<p>And then create a device object for it:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dev = torch.device(</span><br><span class="line">    <span class="string">&quot;cuda&quot;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>Let's update <code>preprocess</code> to move batches to the GPU:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span> x.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>).to(dev), y.to(dev)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_dl, valid_dl = get_data(train_ds, valid_ds, bs)</span><br><span class="line">train_dl = WrappedDataLoader(train_dl, preprocess)</span><br><span class="line">valid_dl = WrappedDataLoader(valid_dl, preprocess)</span><br></pre></td></tr></table></figure>
<p>Finally, we can move our model to the GPU.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.to(dev)</span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<p>You should find it runs faster now:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>
<h2 id="closing-thoughts">Closing thoughts</h2>
<p>We now have a general data pipeline and training loop which you can
use for training many types of models using Pytorch. To see how simple
training a model can now be, take a look at the <a href="https://github.com/fastai/fastai_dev/blob/master/dev_nb/mnist_sample.ipynb">mnist_sample
notebook</a>_.</p>
<p>Of course, there are many things you'll want to add, such as data
augmentation, hyperparameter tuning, monitoring training, transfer
learning, and so forth. These features are available in the fastai
library, which has been developed using the same design approach shown
in this tutorial, providing a natural next step for practitioners
looking to take their models further.</p>
<p>We promised at the start of this tutorial we'd explain through
example each of <code>torch.nn</code>, <code>torch.optim</code>,
<code>Dataset</code>, and <code>DataLoader</code>. So let's summarize
what we've seen:</p>
<ul>
<li><p><code>torch.nn</code>:</p>
<ul>
<li><code>Module</code>: creates a callable which behaves like a
function, but can also contain state(such as neural net layer weights).
It knows what <code>Parameter</code> (s) it contains and can zero all
their gradients, loop through them for weight updates, etc.</li>
<li><code>Parameter</code>: a wrapper for a tensor that tells a
<code>Module</code> that it has weights that need updating during
backprop. Only tensors with the <code>requires_grad</code> attribute set
are updated</li>
<li><code>functional</code>: a module(usually imported into the
<code>F</code> namespace by convention) which contains activation
functions, loss functions, etc, as well as non-stateful versions of
layers such as convolutional and linear layers.</li>
</ul></li>
<li><p><code>torch.optim</code>: Contains optimizers such as
<code>SGD</code>, which update the weights of <code>Parameter</code>
during the backward step</p></li>
<li><p><code>Dataset</code>: An abstract interface of objects with a
<code>__len__</code> and a <code>__getitem__</code>, including classes
provided with Pytorch such as <code>TensorDataset</code></p></li>
<li><p><code>DataLoader</code>: Takes any <code>Dataset</code> and
creates an iterator which returns batches of data.</p></li>
</ul>
]]></content>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>resnet18在cifar10数据集上训练</title>
    <url>/2023/07/15/resnet18%E5%9C%A8cifar10%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E8%AE%AD%E7%BB%83/</url>
    <content><![CDATA[<p>这篇文章是我入门pytorch的第一个实验，实验torchvision中的resnet18在CIFAR数据集上进行训练
<span id="more"></span> # 使用ResNet18在CIFAR10上训练</p>
<h2 id="处理数据集">处理数据集</h2>
<p>这里先处理data_batch_1中的一万个数据
按照官网中的说明使用unpickle函数进行读取,然后使用TensorDataset进行封装。
另外这里有一个细节，由于数据集中我们的训练数据是五万张图像，如果读取出来的五万张图像的List直接转为张量，那么速度会很慢，我们先使用numpy.array()将ndarray的list转化为single
ndarray，转为张量的过程就会显著加速。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">PATH = <span class="string">&#x27;ianafp/CIFAR10/cifar-10-batches-py/data_batch_&#x27;</span></span><br><span class="line">TEST_BATCH = <span class="string">&#x27;ianafp/CIFAR10/cifar-10-batches-py/test_batch&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">unpickle</span>(<span class="params">file</span>):</span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> fo:</span><br><span class="line">        <span class="built_in">dict</span> = pickle.load(fo, encoding=<span class="string">&#x27;bytes&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span></span><br><span class="line">x_train,y_train = [],[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">6</span>):</span><br><span class="line">    dataset = unpickle(PATH+<span class="string">&#x27;&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">    <span class="comment"># print(dataset.keys())</span></span><br><span class="line">    x_train.extend(dataset[<span class="string">b&#x27;data&#x27;</span>])</span><br><span class="line">    y_train.extend(dataset[<span class="string">b&#x27;labels&#x27;</span>])</span><br><span class="line">dataset = unpickle(TEST_BATCH)</span><br><span class="line">x_valid,y_valid = dataset[<span class="string">b&#x27;data&#x27;</span>],dataset[<span class="string">b&#x27;labels&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>测试数据有效性</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(x_train.__len__())</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line">img = np.reshape(x_valid[<span class="number">8000</span>],(<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line">img = np.transpose(img,(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>))</span><br><span class="line">pyplot.imshow(img)</span><br></pre></td></tr></table></figure>
<p>将数据使用dataset和dataloader类进行封装</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset,DataLoader</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x_train.__len__())</span><br><span class="line">batch_size = <span class="number">256</span> </span><br><span class="line">x_train = np.array(x_train)</span><br><span class="line">y_train = np.array(y_train)</span><br><span class="line">x_valid = np.array(x_valid)</span><br><span class="line">y_valid = np.array(y_valid)</span><br><span class="line">x_train = np.reshape(x_train,(x_train.__len__(),<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line">x_valid = np.reshape(x_valid,(x_valid.__len__(),<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line">train_ds = TensorDataset(torch.tensor(x_train,dtype=torch.<span class="built_in">float</span>),torch.tensor(y_train,dtype=torch.<span class="built_in">float</span>))</span><br><span class="line">train_dl = DataLoader(train_ds,batch_size=batch_size)</span><br><span class="line">valid_ds = TensorDataset(torch.tensor(x_valid,dtype=torch.<span class="built_in">float</span>),torch.tensor(y_valid,dtype=torch.<span class="built_in">float</span>))</span><br><span class="line">valid_dl = DataLoader(valid_ds,batch_size=batch_size*<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h2 id="使用torchvison中的resnet18">使用torchvison中的ResNet18</h2>
<p>这里我们使用torchvision中的ResNet model</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> resnet18,ResNet18_Weights</span><br><span class="line">model = resnet18()</span><br></pre></td></tr></table></figure>
<p>设置损失函数，这里我们使用交叉熵损失函数(croos entropy error) <span class="math display">\[
E = - \sum_k t_k \log y_k
\]</span> 其中，<span class="math inline">\(t_k\)</span>为label,<span class="math inline">\(y_k\)</span>为网络输出值
这里实现使用torch.nn.functional中封装好的交叉熵函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">loss_func = F.cross_entropy</span><br></pre></td></tr></table></figure>
<p>实现一个准确率函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">out, yb</span>):</span><br><span class="line">    preds = torch.argmax(out, dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (preds == yb).<span class="built_in">float</span>().mean()</span><br></pre></td></tr></table></figure>
<p>在训练前检验我们模型的损失函数值和准确率。
这里面要注意的点是pytorch卷积层是以浮点数工作的，而我们从图像中读取的训练数据集是byte类型的，因而我们要对tensor作类型转化。
另外torch.nn.functional中的交叉熵损失函数F.cross_entropy中标签以long类型工作，因而也需要作类型转化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x,y = train_ds[<span class="number">0</span>:batch_size]</span><br><span class="line">pred = model(x)</span><br><span class="line"><span class="built_in">print</span>(pred.dtype,y.dtype)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;loss_func = &#x27;</span>,loss_func(pred,y.long()))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;accuracy = &#x27;</span>,accuracy(pred,y))</span><br></pre></td></tr></table></figure>
<p>使用torch.optim进行梯度下降优化</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line">learning_rate = <span class="number">0.5</span></span><br><span class="line">opt = optim.SGD(model.parameters(),lr=learning_rate)</span><br><span class="line">epochs = <span class="number">8</span></span><br></pre></td></tr></table></figure>
<p>定义随机梯度下降函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">loss_batch</span>(<span class="params">model,loss_func,xb,yb,opt=<span class="literal">None</span></span>):</span><br><span class="line">    loss = loss_func(model(xb),yb.long())</span><br><span class="line">    <span class="keyword">if</span> opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line">    <span class="keyword">return</span> loss.item(),<span class="built_in">len</span>(xb)</span><br></pre></td></tr></table></figure>
<p>定义fit函数，fit函数中完成训练过程</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># def fit(epochs,model,loss_func,opt,train_dl,valid_dl):</span></span><br><span class="line"><span class="comment">#     for epoch in range(epochs):</span></span><br><span class="line"><span class="comment">#         model.train()</span></span><br><span class="line"><span class="comment">#         for xb,yb in train_dl:</span></span><br><span class="line"><span class="comment">#             loss_batch(model,loss_func,xb,yb.long(),opt)</span></span><br><span class="line"><span class="comment">#         model.eval()</span></span><br><span class="line"><span class="comment">#         with torch.no_grad():</span></span><br><span class="line"><span class="comment">#             losses,nums = zip(*[loss_batch(model,loss_func,xb,yb) for xb,yb in valid_dl])</span></span><br><span class="line"><span class="comment">#         val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)</span></span><br><span class="line"><span class="comment">#         print(epoch,val_loss)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">epochs,model,loss_func,opt,train_dl,valid_dl</span>):</span><br><span class="line">    epoch = <span class="number">0</span></span><br><span class="line">    pre_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">for</span> xb,yb <span class="keyword">in</span> train_dl:</span><br><span class="line">            loss_batch(model,loss_func,xb,yb.long(),opt)</span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            losses,nums = <span class="built_in">zip</span>(*[loss_batch(model,loss_func,xb,yb) <span class="keyword">for</span> xb,yb <span class="keyword">in</span> valid_dl])</span><br><span class="line">        val_loss = np.<span class="built_in">sum</span>(np.multiply(losses,nums)) / np.<span class="built_in">sum</span>(nums)</span><br><span class="line">        <span class="built_in">print</span>(epoch,val_loss)</span><br><span class="line">        <span class="keyword">import</span> math </span><br><span class="line">        <span class="keyword">if</span> math.fabs(val_loss-pre_loss)&lt;<span class="number">1e-9</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        pre_loss = val_loss</span><br><span class="line">        epoch = epoch + <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>试运行训练函数fit</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># fit(epochs,model,loss_func,opt,train_dl,valid_dl)</span></span><br></pre></td></tr></table></figure>
<p>运行时间过长。</p>
<h2 id="将模型部署在gpu上">将模型部署在GPU上</h2>
<p>检验cuda是否可用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())</span><br><span class="line">dev = torch.device(<span class="string">&#x27;cuda&#x27;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>将数据集迁移到GPU
这里使用的技巧是定义了WrappedDataLoader类，该类是可迭代的，因为定义了__iter__方法。
又因为__iter__方法中使用了关键字yield，因而该类可做生成器(generater)。
在后续 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> train_dl:</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure> 的过程中会动态的将数据应用preprocess方法返回。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">x,y</span>):</span><br><span class="line">    <span class="keyword">return</span> x.to(dev),y.to(dev)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WrappedDataLoader</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dl, func</span>):</span><br><span class="line">        self.dl = dl</span><br><span class="line">        self.func = func</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.dl)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> b <span class="keyword">in</span> self.dl:</span><br><span class="line">            <span class="keyword">yield</span> (self.func(*b))</span><br><span class="line">train_dl = WrappedDataLoader(train_dl,preprocess)</span><br><span class="line">valid_dl = WrappedDataLoader(valid_dl,preprocess)</span><br></pre></td></tr></table></figure>
<p>将模型迁移到GPU</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.to(dev)</span><br><span class="line">opt = optim.SGD(model.parameters(), lr=learning_rate, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<p>试运行训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>
<p>检验准确率</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x,y = valid_ds[<span class="number">0</span>:batch_size]</span><br><span class="line">x = x.to(dev)</span><br><span class="line">y = y.to(dev)</span><br><span class="line">pred = model(x)</span><br><span class="line"><span class="built_in">print</span>(pred.dtype,y.dtype)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;loss_func = &#x27;</span>,loss_func(pred,y.long()))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;accuracy = &#x27;</span>,accuracy(pred,y))</span><br></pre></td></tr></table></figure>
<p>运行结果表明，固定训练轮次准确率较低，若以<span class="math inline">\(1\times
10^{-9}\)</span>为损失函数的收敛阈值，则运行无法收敛，具体原因暂未得知。</p>
]]></content>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>test-function</title>
    <url>/2023/07/11/test-function/</url>
    <content><![CDATA[本篇用于测试markdown功能 <span id="more"></span> <figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">iframe</span> <span class="attr">src</span> = <span class="string">&quot;./vid.mp4&quot;</span> &gt;</span>测试插入视频<span class="tag">&lt;/<span class="name">iframe</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">style</span>=<span class="string">&quot;position: relative; width: 100%; height: 0; padding-bottom: 75%;&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">iframe</span> </span></span><br><span class="line"><span class="tag"><span class="attr">src</span>=<span class="string">&quot;/vid.mp4&quot;</span> <span class="attr">scrolling</span>=<span class="string">&quot;no&quot;</span> <span class="attr">border</span>=<span class="string">&quot;0&quot;</span> </span></span><br><span class="line"><span class="tag"><span class="attr">frameborder</span>=<span class="string">&quot;no&quot;</span> <span class="attr">framespacing</span>=<span class="string">&quot;0&quot;</span> <span class="attr">allowfullscreen</span>=<span class="string">&quot;true&quot;</span> <span class="attr">style</span>=<span class="string">&quot;position: absolute; width: 100%; </span></span></span><br><span class="line"><span class="string"><span class="tag">height: 100%; left: 0; top: 0;&quot;</span>&gt;</span> <span class="tag">&lt;/<span class="name">iframe</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure>
<iframe src="./vid.mp4">测试插入视频</iframe>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="/vid.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; 
height: 100%; left: 0; top: 0;">
</iframe>
</div>
]]></content>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习入门笔记1</title>
    <url>/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/</url>
    <content><![CDATA[<hr>
<p>本文记录了本人阅读《深度学习入门：基于Python的理论与实现》（斋藤康毅）一书做的一些笔记。</p>
<span id="more"></span>
<h2 id="感知机">1 感知机</h2>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708131745739.png"></p>
<p>感知机接收多个输入信号，输出一个信号。这里所说的“信号”可以想
象成电流或河流那样具备“流动性”的东西。像电流流过导线，向前方输送
电子一样，感知机的信号也会形成流，向前方输送信息。但是，和实际的电
流不同的是，感知机的信号只有“流/不 流 ”（
1/0）两种取值。在《深度学习入门：基于Python的理论与实现》中，0
对应“不传递信号”，1对应“传递信号”。</p>
<p>而<span class="math inline">\(w_1,w_2\)</span>是信号的权重，感知机的工作模式可以用以下公式描述。
<span class="math display">\[
y =
\begin{cases}
0, \ w_1x_1+w_2x_2 &lt;= \theta \\
1, \ otherwise
\end{cases}
\]</span> 此处的<span class="math inline">\(\theta\)</span>应理解为一个阈值。</p>
<p>上式也可写作 <span class="math display">\[
y = \begin{cases}0, b + w_1x_1+w_2x_2 &lt;= 0 \\1, \
otherwise\end{cases}
\]</span> 其中<span class="math inline">\(b\)</span>称为偏差,
反应感知机易于激活的程度。</p>
<p>也可以将感知机中的偏差<span class="math inline">\(b\)</span>显示地表达出来</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708160702807.png" alt="image-20230708160702807">
<figcaption aria-hidden="true">image-20230708160702807</figcaption>
</figure>
<p>将上式进一步简化后可表达为 <span class="math display">\[
y = h(b+w_1x_1+w_2x_2) \\
h(x)= \begin{cases}
0, x \le 0 \\
1, x &gt; 1
\end{cases}
\]</span></p>
<h2 id="神经网络">2 神经网络</h2>
<h3 id="神经网络层">2.1 神经网络层</h3>
<p>一个神经网络的例子，本书中将输入层，中间层，输出层依次作为0，1，2层</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708161048708.png" alt="image-20230708161048708">
<figcaption aria-hidden="true">image-20230708161048708</figcaption>
</figure>
<h3 id="激活函数">2.2 激活函数</h3>
<p>在感知机部分中我们引入过<span class="math inline">\(h(x)\)</span>,
这类函数一般都是将输入信号总和转为一个输出信号。</p>
<p>在感知机中的工作模式如下 <span class="math display">\[
a = b + w_1x_1+w_2x_2  \\
y = h(a)
\]</span> 在感知机中显示得表达出<span class="math inline">\(h(x)\)</span>的工作</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708161520990.png" alt="image-20230708161520990">
<figcaption aria-hidden="true">image-20230708161520990</figcaption>
</figure>
<h4 id="sigmod函数">2.2.1 sigmod函数</h4>
<p><span class="math display">\[
h(x) = \frac{1}{1+e^{-x}}
\]</span></p>
<p>sigmod函数是一种常见的激活函数。</p>
<h4 id="阶跃函数">2.2.2 阶跃函数</h4>
<p>阶跃函数很简单，就是根据一个阈值将连续输入转为离散化的输出，和第一部分感知机中讲述的<span class="math inline">\(h(x)\)</span>
是一样的，类似于我们在学微积分时候的sign函数。</p>
<p>以下是阶跃函数和sigmod函数的图像对比。</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708162025130.png" alt="image-20230708162025130">
<figcaption aria-hidden="true">image-20230708162025130</figcaption>
</figure>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708162036814.png" alt="image-20230708162036814">
<figcaption aria-hidden="true">image-20230708162036814</figcaption>
</figure>
<h4 id="relu">2.2.3 ReLU</h4>
<p>ReLU函数是近来使用较多的函数。 <span class="math display">\[
h(x) = \begin{cases}
x, x\ge 0 \\
0, x &lt; 0
\end{cases}
\]</span> <img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708162234505.png" alt="image-20230708162234505"></p>
<h3 id="神经网络的内积">2.3 神经网络的内积</h3>
<p>神经网络的运算基础是通过矩阵的乘法，示意图如下</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708163116056.png" alt="image-20230708163116056">
<figcaption aria-hidden="true">image-20230708163116056</figcaption>
</figure>
<p>多层神经网络如下图</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708163211477.png" alt="image-20230708163211477">
<figcaption aria-hidden="true">image-20230708163211477</figcaption>
</figure>
<h2 id="神经网络的学习">3 神经网络的学习</h2>
<p>前面介绍了神经网络的工作模式，不难看出，保证神经网络正确工作（完成识别，分类等任务）的关键在于权重和神经元的数量。</p>
<p>神经网络的关键点就在于如何从数据中学习，获得正确的参数。</p>
<h3 id="训练数据和测试数据">训练数据和测试数据</h3>
<p>机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和
实验等。首先，使用训练数据进行学习，寻找最优的参数；然后，使用测试
数据评价训练得到的模型的实际能力。</p>
<p>为什么需要将数据分为训练数据和测
试数据呢？因为我们追求的是模型的泛化能力。为了正确评价模型的泛化能
力，就必须划分训练数据和测试数据。另外，训练数据也可以称为监督数据。</p>
<p>仅仅用一个数据集去学习和评价参数，是无法进行正确评价的。
这样会导致可以顺利地处理某个数据集，但无法处理其他数据集的情况。顺
便说一下，只对某个数据集过度拟合的状态称为过拟合（over fitting）。</p>
<h3 id="损失函数">损失函数</h3>
<p>损失函数(loss
function)描述的是神经网络性能的恶劣程度,一般使用均方误差和交叉熵。</p>
<h4 id="均方误差">均方误差</h4>
<p><span class="math display">\[
E = \frac{1}{2}\sum_k(y_k-t_k)^2
\]</span></p>
<p>其中,<span class="math inline">\(y_k,t_k\)</span>分别表示神经网络的输出，监督数据，神经网络的输出和监督数据的差异的均方体现为均方误差。</p>
<h4 id="交叉熵误差">交叉熵误差</h4>
<p><span class="math display">\[
E = - \sum _k t_k\log y_k
\]</span></p>
<p>采用交叉熵策略的道理是一般监督数据<span class="math inline">\(t_k\)</span>往往采用独热码(one
hot)，因此在正确标签的位是1，其余为0。</p>
<p>因此交叉熵误差事实上是测试的是在监督数据输出为正确的情况下，神经网络输出的误差。</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710144238609.png" alt="image-20230710144238609">
<figcaption aria-hidden="true">image-20230710144238609</figcaption>
</figure>
<p>根据自然对数图像，很自然地，神经网络输出值越接近1（即正确值），交叉熵误差就越小。</p>
<h3 id="mini-batch">mini-batch</h3>
<p>使用mini-batch的理由很自然，神经网络的训练数据集往往规模巨大，如果以全部数据为对象计算损失函数是成本巨大的。</p>
<p>因此我们往往从数据集中选出一部分来作为全部数据的近似，称为mini-batch,即小批量。</p>
<h3 id="梯度法">梯度法</h3>
<p>关于函数，导数，偏导数，数值微分，梯度的概念自行查阅微积分资料。</p>
<p>机器学习的主要任务就是在学习时寻找最优参数（权重和偏置）</p>
<p>我们可以将神经网络的损失函数<span class="math inline">\(L\)</span>看作是关于权重的多元函数. <span class="math display">\[
L = f(w_1,w_2,...,w_n)
\]</span> 一般而言，神经网络参数空间巨大，<span class="math inline">\(L\)</span>
往往无法得到解析的表示，而我们只需要得到<span class="math inline">\(L\)</span>的极小值点就可以达到优化的目的，我们认为<span class="math inline">\(L\)</span> 关于参数空间是连续且可微。</p>
<p>那么我们可以利用数值微分的方法得到<span class="math inline">\(L\)</span>关于某参数<span class="math inline">\(w_k\)</span> 的数值微分 <span class="math display">\[
\frac{\partial L}{\partial w_k} = \frac{L(w_k+\delta)-L(w_k)}{\delta}
\]</span> 进而我们得到L的梯度 <span class="math display">\[
grad(L) = (\frac{\partial L}{\partial w_1},\frac{\partial L}{\partial
w_2},\frac{\partial L}{\partial w_3},...,\frac{\partial L}{\partial
w_n})
\]</span> 利用梯度下降进行优化的迭代步骤如下: <span class="math display">\[
W = W-\eta \frac{\partial L}{\partial W}
\]</span> 其中<span class="math inline">\(\eta\)</span>为学习率，即权重向梯度方向迭代的步长。</p>
<h3 id="总结">总结</h3>
<p>总结神经网络的学习算法，步骤如下：</p>
<ol type="1">
<li>mini-batch</li>
<li>计算梯度</li>
<li>更新参数</li>
<li>重复步骤1，2，3到阈值</li>
</ol>
<p>由于使用的数据是随机的mini-batch数据，因此上述方法又称为随机梯度下降法。</p>
<h2 id="误差反向传播法">误差反向传播法</h2>
<h3 id="计算图">计算图</h3>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710152529536.png" alt="image-20230710152529536">
<figcaption aria-hidden="true">image-20230710152529536</figcaption>
</figure>
<p>这是一个最简单的计算图，描述的是买苹果的过程中，在苹果数量，单价，消费税三者作用下如何得到最终开销。</p>
<p>使用计算图描述计算过程的好处一是能够进行局部计算，二是能够通过误差反向传播法求导数。</p>
<h3 id="利用误差反向传播求导">利用误差反向传播求导</h3>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710152716600.png" alt="image-20230710152716600">
<figcaption aria-hidden="true">image-20230710152716600</figcaption>
</figure>
<p>上图是一个用误差反向传播求出导数的例子（苹果的单价，总价对最终价格影响的贡献）。</p>
<h4 id="链式法则">链式法则</h4>
<p>利用计算图进行反向传播计算导数的过程是从右向左传递，与我们日常接触的计算恰恰相反。</p>
<p>传递导数的原理在于<strong>链式法则</strong>。</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710153004628.png" alt="image-20230710153004628">
<figcaption aria-hidden="true">image-20230710153004628</figcaption>
</figure>
<p>假设存在<span class="math inline">\(y=f(x)\)</span>的计算，反向传播如上图所示。</p>
<p><span class="math inline">\(E\)</span>信号是右侧传来的信号，乘以<span class="math inline">\(f\)</span>节点的局部导数<span class="math inline">\(\frac{\partial y}{\partial x}\)</span>
后作为新的导数信号向左侧传递。</p>
<p>事实上可以看出反向传播的过程遵循链式法则。</p>
<p>即<span class="math inline">\(z = f(t),t=f(x)\)</span>,那么$ = $</p>
<p>链式法则是复合函数求导的性质，具体查阅微积分教材。</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710154633453.png" alt="image-20230710154633453">
<figcaption aria-hidden="true">image-20230710154633453</figcaption>
</figure>
<p>使用链式法则表示的计算图过程如上。</p>
<h4 id="加法节点的反向传播">加法节点的反向传播</h4>
<p>以<span class="math inline">\(z = x+y\)</span>为对象，那么<span class="math inline">\(\frac{\partial z}{\partial x} = \frac{\partial
z}{\partial y} = 1\)</span></p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710154858929.png" alt="image-20230710154858929">
<figcaption aria-hidden="true">image-20230710154858929</figcaption>
</figure>
<h4 id="乘法节点的反向传播">乘法节点的反向传播</h4>
<p>以<span class="math inline">\(z = xy\)</span>为对象，那么<span class="math inline">\(\frac{\partial z}{\partial x} =y, \frac{\partial
z}{\partial y} = x\)</span></p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710154935192.png" alt="image-20230710154935192">
<figcaption aria-hidden="true">image-20230710154935192</figcaption>
</figure>
<h4 id="实例代码">实例代码</h4>
<p>有反向传播机制的加法层和乘法层。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AddLayer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        out = x + y</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dout</span>):</span><br><span class="line">        dx = dout * <span class="number">1</span></span><br><span class="line">        dy = dout * <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> dx, dy</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MulLayer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.x = <span class="literal">None</span></span><br><span class="line">        self.y = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        self.x = x</span><br><span class="line">        self.y = y</span><br><span class="line">        out = x * y</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dout</span>):</span><br><span class="line">        dx = dout * self.y <span class="comment"># 翻转x和y</span></span><br><span class="line">        dy = dout * self.x</span><br><span class="line">        <span class="keyword">return</span> dx, dy</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="考虑激活层的情形">考虑激活层的情形</h4>
<p>考虑ReLU <span class="math display">\[
y=\begin{cases}
x, x&gt;0 \\
0, x\le 0
\end{cases}
\]</span> 那么， <span class="math display">\[
\frac{\partial y}{\partial x} = \begin{cases}
1, x&gt;0 \\
0, x\le 0
\end{cases}
\]</span> 对于sigmoid, <span class="math display">\[
y = \frac{1}{1+e^{-x}} \\
\frac{dy}{dx} = - (\frac{1}{1+e^{-x}})^2 \cdot (-e^{-x}) =
\frac{e^{-x}}{(1+e^{-x})^2} = y(1-y)
\]</span> <img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710155735826.png" alt="image-20230710155735826"></p>
<h4 id="考虑affine层的实现">考虑Affine层的实现</h4>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710162332961.png" alt="image-20230710162332961">
<figcaption aria-hidden="true">image-20230710162332961</figcaption>
</figure>
<p>Affine层实际上就是考虑了矩阵运算的清醒，事实上对于矩阵运算的反向传播，有下式
<span class="math display">\[
\frac{\partial L}{\partial X} = \frac{\partial L}{\partial Y} \cdot W^T
\\
\frac{\partial L}{\partial W} = X^T \cdot \frac{\partial L}{\partial Y}
\]</span> <img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710162647105.png" alt="image-20230710162647105"></p>
<h4 id="使用误差反向传播法的学习">使用误差反向传播法的学习</h4>
<p>两层神经网络层的实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TwoLayerNet</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size,</span></span><br><span class="line"><span class="params">                 weight_init_std=<span class="number">0.01</span></span>):</span><br><span class="line">        <span class="comment"># 初始化权重</span></span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        self.params[<span class="string">&#x27;W1&#x27;</span>] = weight_init_std * \</span><br><span class="line">                            np.random.randn(input_size, hidden_size)</span><br><span class="line">        self.params[<span class="string">&#x27;b1&#x27;</span>] = np.zeros(hidden_size)</span><br><span class="line">        self.params[<span class="string">&#x27;W2&#x27;</span>] = weight_init_std * \</span><br><span class="line">                            np.random.randn(hidden_size, output_size)</span><br><span class="line">        self.params[<span class="string">&#x27;b2&#x27;</span>] = np.zeros(output_size)</span><br><span class="line">        <span class="comment"># 生成层</span></span><br><span class="line">        self.layers = OrderedDict()</span><br><span class="line">        self.layers[<span class="string">&#x27;Affine1&#x27;</span>] = \</span><br><span class="line">            Affine(self.params[<span class="string">&#x27;W1&#x27;</span>], self.params[<span class="string">&#x27;b1&#x27;</span>])</span><br><span class="line">        self.layers[<span class="string">&#x27;Relu1&#x27;</span>] = Relu()</span><br><span class="line">        self.layers[<span class="string">&#x27;Affine2&#x27;</span>] = \</span><br><span class="line">            Affine(self.params[<span class="string">&#x27;W2&#x27;</span>], self.params[<span class="string">&#x27;b2&#x27;</span>])</span><br><span class="line">        self.lastLayer = SoftmaxWithLoss()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers.values():</span><br><span class="line">            x = layer.forward(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="comment"># x: 输入数据, t:监督数据</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        y = self.predict(x)</span><br><span class="line">        <span class="keyword">return</span> self.lastLayer.forward(y, t)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        y = self.predict(x)</span><br><span class="line">        y = np.argmax(y, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> t.ndim != <span class="number">1</span> : t = np.argmax(t, axis=<span class="number">1</span>)</span><br><span class="line">accuracy = np.<span class="built_in">sum</span>(y == t) / <span class="built_in">float</span>(x.shape[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">return</span> accuracy</span><br><span class="line">    <span class="comment"># x: 输入数据, t:监督数据</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">numerical_gradient</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        loss_W = <span class="keyword">lambda</span> W: self.loss(x, t)</span><br><span class="line">        grads = &#123;&#125;</span><br><span class="line">        grads[<span class="string">&#x27;W1&#x27;</span>] = numerical_gradient(loss_W, self.params[<span class="string">&#x27;W1&#x27;</span>])</span><br><span class="line">        grads[<span class="string">&#x27;b1&#x27;</span>] = numerical_gradient(loss_W, self.params[<span class="string">&#x27;b1&#x27;</span>])</span><br><span class="line">        grads[<span class="string">&#x27;W2&#x27;</span>] = numerical_gradient(loss_W, self.params[<span class="string">&#x27;W2&#x27;</span>])</span><br><span class="line">        grads[<span class="string">&#x27;b2&#x27;</span>] = numerical_gradient(loss_W, self.params[<span class="string">&#x27;b2&#x27;</span>])</span><br><span class="line">        <span class="keyword">return</span> grads</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        <span class="comment"># forward</span></span><br><span class="line">        self.loss(x, t)</span><br><span class="line">        <span class="comment"># backward</span></span><br><span class="line">        dout = <span class="number">1</span></span><br><span class="line">        dout = self.lastLayer.backward(dout)</span><br><span class="line">        layers = <span class="built_in">list</span>(self.layers.values())</span><br><span class="line">        layers.reverse()</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> layers:</span><br><span class="line">            dout = layer.backward(dout)</span><br><span class="line">        <span class="comment">#设定 </span></span><br><span class="line">        grads = &#123;&#125;</span><br><span class="line">        grads[<span class="string">&#x27;W1&#x27;</span>] = self.layers[<span class="string">&#x27;Affine1&#x27;</span>].dW</span><br><span class="line">        grads[<span class="string">&#x27;b1&#x27;</span>] = self.layers[<span class="string">&#x27;Affine1&#x27;</span>].db</span><br><span class="line">        grads[<span class="string">&#x27;W2&#x27;</span>] = self.layers[<span class="string">&#x27;Affine2&#x27;</span>].dW</span><br><span class="line">        grads[<span class="string">&#x27;b2&#x27;</span>] = self.layers[<span class="string">&#x27;Affine2&#x27;</span>].db</span><br><span class="line">        <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>
<p>将前文中利用梯度下降求梯度的方法替换为使用误差反向传播。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys, os</span><br><span class="line"> sys.path.append(os.pardir)</span><br><span class="line"> <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> <span class="keyword">from</span> dataset.mnist <span class="keyword">import</span> load_mnist</span><br><span class="line"> <span class="keyword">from</span> two_layer_net <span class="keyword">import</span> TwoLayerNet</span><br><span class="line"> <span class="comment"># 读入数据</span></span><br><span class="line">(x_train, t_train), (x_test, t_test) = \</span><br><span class="line">    load_mnist(normalize=<span class="literal">True</span>, one_hot_label=<span class="literal">True</span>)</span><br><span class="line"> network = TwoLayerNet(input_size=<span class="number">784</span>, hidden_size=<span class="number">50</span>, output_size=<span class="number">10</span>)</span><br><span class="line"> iters_num = <span class="number">10000</span></span><br><span class="line"> train_size = x_train.shape[<span class="number">0</span>]</span><br><span class="line"> batch_size = <span class="number">100</span></span><br><span class="line"> learning_rate = <span class="number">0.1</span></span><br><span class="line"> train_loss_list = []</span><br><span class="line"> train_acc_list = []</span><br><span class="line"> test_acc_list = []</span><br><span class="line"> iter_per_epoch = <span class="built_in">max</span>(train_size / batch_size, <span class="number">1</span>)</span><br><span class="line"> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iters_num):</span><br><span class="line">    batch_mask = np.random.choice(train_size, batch_size)</span><br><span class="line">    x_batch = x_train[batch_mask]</span><br><span class="line">    t_batch = t_train[batch_mask]</span><br><span class="line">    <span class="comment"># 通过误差反向传播法求梯度</span></span><br><span class="line">	grad = network.gradient(x_batch, t_batch)</span><br><span class="line">    <span class="comment"># 更新</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> (<span class="string">&#x27;W1&#x27;</span>, <span class="string">&#x27;b1&#x27;</span>, <span class="string">&#x27;W2&#x27;</span>, <span class="string">&#x27;b2&#x27;</span>):</span><br><span class="line">        network.params[key] -= learning_rate * grad[key]</span><br><span class="line">    loss = network.loss(x_batch, t_batch)</span><br><span class="line">    train_loss_list.append(loss)</span><br><span class="line">    <span class="keyword">if</span> i % iter_per_epoch == <span class="number">0</span>:</span><br><span class="line">        train_acc = network.accuracy(x_train, t_train)</span><br><span class="line">        test_acc = network.accuracy(x_test, t_test)</span><br><span class="line">        train_acc_list.append(train_acc)</span><br><span class="line">        test_acc_list.append(test_acc)</span><br><span class="line">        <span class="built_in">print</span>(train_acc, test_acc)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="卷积神经网络">卷积神经网络</h2>
<p>卷积神经网络即CNN，(Convolutional Neural Network)</p>
<p>常常用于图像识别，语音识别等场合。</p>
<h3 id="整体结构">整体结构</h3>
<p>CNN结构和之前的神经网络结构类似，但是出现了卷积层(convolutional)和池化层(pooling)。</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710163237306.png" alt="image-20230710163237306">
<figcaption aria-hidden="true">image-20230710163237306</figcaption>
</figure>
<p>相邻神经元都有连接的结构称为全连接结构，上图在Affine层实现了全连接。</p>
<p>那么在CNN中增加了卷积层和池化层后则如下图所示。</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710163356084.png" alt="image-20230710163356084">
<figcaption aria-hidden="true">image-20230710163356084</figcaption>
</figure>
<h3 id="卷积运算">卷积运算</h3>
<p>全连接层存在的问题就是全连接层输入时将多维数据拉成一维数据，损失了输入的形状信息，而卷积层则是在保留输入形状信息的基础上进行运算的。</p>
<p>而卷积层可以保持形状不变。当输入数据是图像时，卷积层会以3维
数据的形式接收输入数据，并同样以3维数据的形式输出至下一层。因此，
在CNN中，可以（有可能）正确理解图像等具有形状的数据。</p>
<p>另外，CNN中，有时将卷积层的输入输出数据称为特征图（feature
map）。其中，卷积层的输入数据称为输入特征图（input feature map）， 输 出
数据称为输出特征图（output feature map）。本书中将“输入输出数据”和“特
征图”作为含义相同的词使用。</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710165057328.png" alt="image-20230710165057328">
<figcaption aria-hidden="true">image-20230710165057328</figcaption>
</figure>
<p>上图是一个卷积运算的例子。</p>
<p>卷积运算事实上是滤波器（也称卷积核）与输入数据各个位置上对应元素相乘再相加。</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710165211078.png" alt="image-20230710165211078">
<figcaption aria-hidden="true">image-20230710165211078</figcaption>
</figure>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710165228579.png" alt="image-20230710165228579">
<figcaption aria-hidden="true">image-20230710165228579</figcaption>
</figure>
<p>对于偏置，对所有元素进行偏置。</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710165253407.png" alt="image-20230710165253407">
<figcaption aria-hidden="true">image-20230710165253407</figcaption>
</figure>
<h4 id="填充">填充</h4>
<p>为了控制卷积生成的形状，我们常常要对输入数据先进行填充。</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710165406387.png" alt="image-20230710165406387">
<figcaption aria-hidden="true">image-20230710165406387</figcaption>
</figure>
<h4 id="步幅">步幅</h4>
<p>卷积核在输入数据上移动的步长称为步幅，步幅越大，卷积输出的规模越小。</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710165527994.png" alt="image-20230710165527994">
<figcaption aria-hidden="true">image-20230710165527994</figcaption>
</figure>
<h4 id="多维数据的卷积运算">多维数据的卷积运算</h4>
<p>这里以3通道的数据为例，
展示了卷积运算的结果。和2维数据时相比，可以发现纵深
方向（通道方向）上特征图增加了。通道方向上有多个特征图时，会按通道
进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出。</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710165718840.png" alt="image-20230710165718840">
<figcaption aria-hidden="true">image-20230710165718840</figcaption>
</figure>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710165725537.png" alt="image-20230710165725537">
<figcaption aria-hidden="true">image-20230710165725537</figcaption>
</figure>
<p>那么如何使得生成结果中具有更多维度呢？</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710190037365.png" alt="image-20230710190037365">
<figcaption aria-hidden="true">image-20230710190037365</figcaption>
</figure>
<p>结合方块思考，我们可以将三维矩阵的卷积抽象为上述方块，立方体*立方体得到正方形，</p>
<p>如果想要得到立方体的结果，那么卷积核就要再提高一个维度，也就是将N个卷积核的二维结果拼接成三维。</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710190214544.png" alt="image-20230710190214544">
<figcaption aria-hidden="true">image-20230710190214544</figcaption>
</figure>
<p>如果应用批处理，那么就在输入数据上再增加一个批维度。</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710190353649.png" alt="image-20230710190353649">
<figcaption aria-hidden="true">image-20230710190353649</figcaption>
</figure>
<p>像这样，数据作为4维的形状在各层间传递。这里需要注意的是，网络间传
递的是4维数据，对这N个数据进行了卷积运算。也就是说，批处理将N次
的处理汇总成了1次进行。</p>
<h3 id="池化层">池化层</h3>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710190517677.png" alt="image-20230710190517677">
<figcaption aria-hidden="true">image-20230710190517677</figcaption>
</figure>
<p>池化运算是缩小矩阵规模的运算，上图中将2*2矩阵缩小为1*1,且应用Max池化，即选取矩阵中最大的元素作为池化结果。</p>
<p>除了Max池化之外，还有Average池化等。相对于Max池化是从
目标区域中取出最大值，Average池化则是计算目标区域的平均值。
在图像识别领域，主要使用Max池化。因此，本书中说到“池化层”
时，指的是Max池化。</p>
<p>池化层具有如下特征：</p>
<ul>
<li>没有要学习的参数</li>
<li>通道数不发生改变</li>
<li>对微小变化具有鲁班性，可以理解为应用max池化时，块中非max元素的改变对池化结果没有影响。</li>
</ul>
]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>info</title>
    <url>/2023/07/08/test/</url>
    <content><![CDATA[<p>有什么问题可以在评论区留言，或者email联系ianafp@zju.edu.cn</p>
]]></content>
      <tags>
        <tag>info</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch入门笔记1</title>
    <url>/2023/07/11/pytorch%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/</url>
    <content><![CDATA[<p>本文是一篇pytorch入门笔记，主要讲了Pytorch环境配置和基本数据类型。
<span id="more"></span> ## pytorch的安装</p>
<h3 id="更新cuda驱动">更新cuda驱动</h3>
<p>到<a href="https://www.nvidia.cn/geforce/drivers/">N卡驱动程序官网</a>下载驱动更新程序</p>
<p>可选择自动下载或者手动下载</p>
<figure>
<img src="/2023/07/11/pytorch%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230711105656694.png" alt="image-20230711105656694">
<figcaption aria-hidden="true">image-20230711105656694</figcaption>
</figure>
<p>以自动更新程序为例，在驱动程序页面完成更新即可</p>
<figure>
<img src="/2023/07/11/pytorch%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230711105745926.png" alt="image-20230711105745926">
<figcaption aria-hidden="true">image-20230711105745926</figcaption>
</figure>
<p>更新完成后，查看N卡设置</p>
<figure>
<img src="/2023/07/11/pytorch%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230711105810959.png" alt="image-20230711105810959">
<figcaption aria-hidden="true">image-20230711105810959</figcaption>
</figure>
<figure>
<img src="/2023/07/11/pytorch%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230711105844840.png" alt="image-20230711105844840">
<figcaption aria-hidden="true">image-20230711105844840</figcaption>
</figure>
<p>发现CUDA版本已经更新到12.2.19</p>
<h3 id="下载pytorch">下载pytorch</h3>
<p>在安装了conda的情况下，在命令行输入(windows下powershell需要开管理员权限)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia</span><br></pre></td></tr></table></figure>
<p>在ipython中验证pytorch是否正确安装</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Python 3.10.9 | packaged by Anaconda, Inc. | (main, Mar  1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)] on win32</span><br><span class="line">Type <span class="string">&quot;help&quot;</span>, <span class="string">&quot;copyright&quot;</span>, <span class="string">&quot;credits&quot;</span> or <span class="string">&quot;license&quot;</span> <span class="keyword">for</span> more information.</span><br><span class="line">&gt;&gt;&gt; import torch</span><br><span class="line">&gt;&gt;&gt; x = torch.rand(5,3)</span><br><span class="line">&gt;&gt;&gt; <span class="built_in">print</span>(x)</span><br><span class="line">tensor([[0.4660, 0.0961, 0.7834],</span><br><span class="line">        [0.3614, 0.2262, 0.8813],</span><br><span class="line">        [0.9523, 0.8653, 0.2905],</span><br><span class="line">        [0.4628, 0.0707, 0.6660],</span><br><span class="line">        [0.4278, 0.7403, 0.2035]])</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>
<h3 id="为conda环境配置pytorch">为conda环境配置pytorch</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda create -n MyEnv pytorch</span><br><span class="line">conda activate MyEnv</span><br></pre></td></tr></table></figure>
<figure>
<img src="/2023/07/11/pytorch%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230711152214412.png" alt="image-20230711152214412">
<figcaption aria-hidden="true">image-20230711152214412</figcaption>
</figure>
<p>使用jupyter notebook进行验证。</p>
<h2 id="pytorch的基本使用">pytorch的基本使用</h2>
<p>详情请见<a herf="https://pytorch.org/docs/stable/torch.html">pytorch官方文档</a></p>
<h3 id="张量tensors">张量(Tensors)</h3>
<p>pytorch一大作用是替换numpy，使用张量（tensors)替代numpy的多维数组(ndarrays),前者的优势是可以部署到GPU上来加速向量运算。</p>
<p>必须使用的库是torch</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>
<h4 id="定义张量">定义张量</h4>
<p>用法和numpy.rand,empty,zeros类似</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.rand() </span><br><span class="line">torch.empty() <span class="comment"># torch.new_empty()</span></span><br><span class="line">torch.zeros() <span class="comment"># torch.new_zeros()</span></span><br><span class="line">torch.ones() <span class="comment"># torch.new_ones()</span></span><br></pre></td></tr></table></figure>
<p>可以将python list,numpy ndarray转为tensor</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.tensor([[<span class="number">1.</span>, -<span class="number">1.</span>], [<span class="number">1.</span>, -<span class="number">1.</span>]])</span><br><span class="line">torch.tensor(np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]))</span><br></pre></td></tr></table></figure>
<p>利用原有张量形状创建张量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.randn_like(old_tensor,dtype=...)</span><br></pre></td></tr></table></figure>
<h4 id="张量运算">张量运算</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">A = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">B = torch.tensor([[<span class="number">3</span>,<span class="number">5</span>],[<span class="number">9</span>,<span class="number">11</span>]])</span><br><span class="line"><span class="built_in">print</span>(A,B)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>​ tensor([[1, 2], ​ [3, 4]]) tensor([[ 3, 5], ​ [ 9, 11]]) ​</p>
<h4 id="张量运算-1">张量运算</h4>
<h5 id="加法">加法</h5>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加法</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;user operator + &#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(A+B)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;user add(tensor1,tensor2,[out = tensor3])&#x27;</span>)</span><br><span class="line">C = torch.empty_like(A)</span><br><span class="line"><span class="built_in">print</span>(torch.add(A,B,out = C))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output C&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(C)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;A.add(B)&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(A.add(B))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;oprand after add&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(A,B)</span><br></pre></td></tr></table></figure>
<p>​ user operator + ​ tensor([[ 4, 7], ​ [12, 15]]) ​ user
add(tensor1,tensor2,[out = tensor3]) ​ tensor([[ 4, 7], ​ [12, 15]]) ​
output C ​ tensor([[ 4, 7], ​ [12, 15]]) ​ A.add(B) ​ tensor([[ 4, 7], ​ [12,
15]]) ​ oprand after add ​ tensor([[1, 2], ​ [3, 4]]) tensor([[ 3, 5], ​ [
9, 11]]) ​</p>
<h4 id="cuda张量">cuda张量</h4>
<p>tensor可以通过.to方法部署到不同的计算设备上</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span>)          <span class="comment"># 定义一个 CUDA 设备对象</span></span><br><span class="line">    y = torch.ones_like(A, device=device)  <span class="comment"># 显示创建在 GPU 上的一个 tensor</span></span><br><span class="line">    x = y.to(device)                       <span class="comment"># 也可以采用 .to(&quot;cuda&quot;) </span></span><br><span class="line">    z = x + y</span><br><span class="line">    <span class="built_in">print</span>(z)</span><br><span class="line">    <span class="built_in">print</span>(z.to(<span class="string">&quot;cpu&quot;</span>, torch.double))       <span class="comment"># .to() 方法也可以改变数值类型</span></span><br></pre></td></tr></table></figure>
<h3 id="梯度grad">梯度(grad)</h3>
<p>梯度的概念是机器学习中的重中之重。</p>
<p>在pytorch中，pytorch中的autograd库提供了对tensor所有运算自动跟踪计算梯度的功能。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">A = torch.rand(<span class="number">3</span>,<span class="number">3</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(A)</span><br><span class="line">B = A + <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(B)</span><br><span class="line">C= B*B*<span class="number">3</span></span><br><span class="line"><span class="built_in">print</span>(C)</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure>
<img src="/2023/07/11/pytorch%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230711162307175.png" alt="image-20230711162307175">
<figcaption aria-hidden="true">image-20230711162307175</figcaption>
</figure>
<p>使用backward方法使用反向传播计算A张量的梯度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">out = C.mean()</span><br><span class="line"><span class="built_in">print</span>(out)</span><br><span class="line">out.backward()</span><br><span class="line"><span class="built_in">print</span>(A.grad)</span><br></pre></td></tr></table></figure>
<figure>
<img src="/2023/07/11/pytorch%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230711162845636.png" alt="image-20230711162845636">
<figcaption aria-hidden="true">image-20230711162845636</figcaption>
</figure>
]]></content>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
</search>
