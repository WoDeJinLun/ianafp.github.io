<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>info</title>
    <url>/2023/07/08/test/</url>
    <content><![CDATA[<p>有什么问题可以在评论区留言，或者email联系ianafp@zju.edu.cn</p>
]]></content>
      <tags>
        <tag>info</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习入门笔记1</title>
    <url>/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/</url>
    <content><![CDATA[<hr>
<h2 id="感知机">1 感知机</h2>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708131745739.png"></p>
<p>感知机接收多个输入信号，输出一个信号。这里所说的“信号”可以想
象成电流或河流那样具备“流动性”的东西。像电流流过导线，向前方输送
电子一样，感知机的信号也会形成流，向前方输送信息。但是，和实际的电
流不同的是，感知机的信号只有“流/不 流 ”（
1/0）两种取值。在《深度学习入门：基于Python的理论与实现》中，0
对应“不传递信号”，1对应“传递信号”。</p>
<p>而<span class="math inline">\(w_1,w_2\)</span>是信号的权重，感知机的工作模式可以用以下公式描述。
<span class="math display">\[
y =
\begin{cases}
0, \ w_1x_1+w_2x_2 &lt;= \theta \\
1, \ otherwise
\end{cases}
\]</span> 此处的<span class="math inline">\(\theta\)</span>应理解为一个阈值。</p>
<p>上式也可写作 <span class="math display">\[
y = \begin{cases}0, b + w_1x_1+w_2x_2 &lt;= 0 \\1, \
otherwise\end{cases}
\]</span> 其中<span class="math inline">\(b\)</span>称为偏差,
反应感知机易于激活的程度。</p>
<p>也可以将感知机中的偏差<span class="math inline">\(b\)</span>显示地表达出来</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708160702807.png" alt="image-20230708160702807">
<figcaption aria-hidden="true">image-20230708160702807</figcaption>
</figure>
<p>将上式进一步简化后可表达为 <span class="math display">\[
y = h(b+w_1x_1+w_2x_2) \\
h(x)= \begin{cases}
0, x \le 0 \\
1, x &gt; 1
\end{cases}
\]</span></p>
<h2 id="神经网络">2 神经网络</h2>
<h3 id="神经网络层">2.1 神经网络层</h3>
<p>一个神经网络的例子，本书中将输入层，中间层，输出层依次作为0，1，2层</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708161048708.png" alt="image-20230708161048708">
<figcaption aria-hidden="true">image-20230708161048708</figcaption>
</figure>
<h3 id="激活函数">2.2 激活函数</h3>
<p>在感知机部分中我们引入过<span class="math inline">\(h(x)\)</span>,
这类函数一般都是将输入信号总和转为一个输出信号。</p>
<p>在感知机中的工作模式如下 <span class="math display">\[
a = b + w_1x_1+w_2x_2  \\
y = h(a)
\]</span> 在感知机中显示得表达出<span class="math inline">\(h(x)\)</span>的工作</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708161520990.png" alt="image-20230708161520990">
<figcaption aria-hidden="true">image-20230708161520990</figcaption>
</figure>
<h4 id="sigmod函数">2.2.1 sigmod函数</h4>
<p><span class="math display">\[
h(x) = \frac{1}{1+e^{-x}}
\]</span></p>
<p>sigmod函数是一种常见的激活函数。</p>
<h4 id="阶跃函数">2.2.2 阶跃函数</h4>
<p>阶跃函数很简单，就是根据一个阈值将连续输入转为离散化的输出，和第一部分感知机中讲述的<span class="math inline">\(h(x)\)</span>
是一样的，类似于我们在学微积分时候的sign函数。</p>
<p>以下是阶跃函数和sigmod函数的图像对比。</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708162025130.png" alt="image-20230708162025130">
<figcaption aria-hidden="true">image-20230708162025130</figcaption>
</figure>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708162036814.png" alt="image-20230708162036814">
<figcaption aria-hidden="true">image-20230708162036814</figcaption>
</figure>
<h4 id="relu">2.2.3 ReLU</h4>
<p>ReLU函数是近来使用较多的函数。 <span class="math display">\[
h(x) = \begin{cases}
x, x\ge 0 \\
0, x &lt; 0
\end{cases}
\]</span> <img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708162234505.png" alt="image-20230708162234505"></p>
<h3 id="神经网络的内积">2.3 神经网络的内积</h3>
<p>神经网络的运算基础是通过矩阵的乘法，示意图如下</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708163116056.png" alt="image-20230708163116056">
<figcaption aria-hidden="true">image-20230708163116056</figcaption>
</figure>
<p>多层神经网络如下图</p>
<figure>
<img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708163211477.png" alt="image-20230708163211477">
<figcaption aria-hidden="true">image-20230708163211477</figcaption>
</figure>
<h2 id="神经网络的学习">3 神经网络的学习</h2>
<p>前面介绍了神经网络的工作模式，不难看出，保证神经网络正确工作（完成识别，分类等任务）的关键在于权重和神经元的数量。</p>
<p>神经网络的关键点就在于如何从数据中学习，获得正确的参数。</p>
]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2023/07/08/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very
first post. Check <a href="https://hexo.io/docs/">documentation</a> for
more info. If you get any problems when using Hexo, you can find the
answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or
you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <tags>
        <tag>博客使用说明</tag>
      </tags>
  </entry>
</search>
