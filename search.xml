<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>PaperReading:Efficientnet: Rethinking model scaling for convolutional neural networks</title>
    <url>/2023/07/21/PaperReading-Efficientnet-Rethinking-model-scaling-for-convolutional-neural-networks/</url>
    <content><![CDATA[<p>这一篇文章是我阅读论文《Efficientnet: Rethinking model scaling for convolutional neural》的记录。    </p>
<span id="more"></span>
<h2 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h2><p>摘要部分说明了该论文的内容，本论文提出一种模型缩放的方式并应用于MobileNet和ResNet上以证明有效性, 更进一步，本文设计出了Efficient Nets, 相比于传统的CNN具有更好的效率和准确率。</p>
<p>源码于:</p>
<p><a href="https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet">tpu/models/official/efficientnet at master · tensorflow/tpu (github.com)</a></p>
<h2 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h2><p><img src="/2023/07/21/PaperReading-Efficientnet-Rethinking-model-scaling-for-convolutional-neural-networks/image-20230721152752404.png" alt="image-20230721152752404"></p>
<p>introduction部分的主题思想大概是他们初步的经验化的找到了量化网络的深度宽度和分辨率的方法，即当计算资源扩大$2^N$倍时，网络的深度，宽度，以及分辨率分别扩大$\alpha ^N,\beta ^N,\gamma ^N $倍数 ,而$\alpha,\beta,\gamma$分别由原始网络上的a small grid search决定。</p>
<blockquote>
<p>For example, if we want to use 2N times more computational resources, then we can simply increase the network depth by  αN , width by βN , and image size by γN , where α, β, γ are constant coefficients determined by a small grid search on the original small model. Figure 2 illustrates the difference between our scaling method and conventional methods</p>
</blockquote>
<p>而模型拓展的效率对原始模型的依赖程度很大，因而作者使用neural architecture search来编写了一些列baseline network, 命名为efficient net</p>
<blockquote>
<p>Notably, the effectiveness of model scaling heavily depends on the baseline network; to<br>go even further, we use neural architecture search (Zoph &amp; Le, 2017; Tan et al., 2019) to develop a new baseline network, and scale it up to obtain a family of models, called EfficientNets. </p>
</blockquote>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="ConvNet-Accuracy"><a href="#ConvNet-Accuracy" class="headerlink" title="ConvNet Accuracy"></a>ConvNet Accuracy</h3><p>目前以增大模型参数规模的方式提升accuracy已经进入瓶颈。</p>
<blockquote>
<p>Notably, the effectiveness of model scaling heavily depends on the baseline network; to<br>go even further, we use neural architecture search (Zoph &amp; Le, 2017; Tan et al., 2019) to develop a new baseline network, and scale it up to obtain a family of models, called<br>EfficientNets. </p>
</blockquote>
<h3 id="ConvNet-Efficiency"><a href="#ConvNet-Efficiency" class="headerlink" title="ConvNet Efficiency"></a>ConvNet Efficiency</h3><p>关于卷积神经网络有许多方法可以进行accuracy和effficiency的tradeoff,因为目前的模型一般都过参数化，导致训练effficeincy很低而对accuracy的提升也很低。</p>
<p>但是减少参数的方法是不易找到的。</p>
<blockquote>
<p>However, it is unclear how to apply these techniques for larger models that have much larger design space and much more expensive tuning cost. In this paper, we aim to study model efficiency for super large ConvNets that surpass state-of-the-art accuracy. To achieve this goal, we resort to model scaling.</p>
</blockquote>
<h3 id="Model-Scaling"><a href="#Model-Scaling" class="headerlink" title="Model Scaling"></a>Model Scaling</h3><p>对于一个模型采取适应不同计算规模的scale有许多方法，比如对于ResNet就可以进行ResNet18到ResNet200之间的伸缩。本文作者将系统化的研究和总结model scaling的方法，关于depth,width,resolution</p>
<h2 id="Compound-Model-Scaling"><a href="#Compound-Model-Scaling" class="headerlink" title="Compound Model Scaling"></a>Compound Model Scaling</h2><h2 id="Efficient-Achitecture"><a href="#Efficient-Achitecture" class="headerlink" title="Efficient Achitecture"></a>Efficient Achitecture</h2><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2>]]></content>
      <tags>
        <tag>PaperReading</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch入门笔记1</title>
    <url>/2023/07/11/pytorch%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/</url>
    <content><![CDATA[<p>本文是一篇pytorch入门笔记，主要讲了Pytorch环境配置和基本数据类型。<br><span id="more"></span></p>
<h2 id="pytorch的安装"><a href="#pytorch的安装" class="headerlink" title="pytorch的安装"></a>pytorch的安装</h2><h3 id="更新cuda驱动"><a href="#更新cuda驱动" class="headerlink" title="更新cuda驱动"></a>更新cuda驱动</h3><p>到<a href="https://www.nvidia.cn/geforce/drivers/">N卡驱动程序官网</a>下载驱动更新程序</p>
<p>可选择自动下载或者手动下载</p>
<p><img src="/2023/07/11/pytorch%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230711105656694.png" alt="image-20230711105656694"></p>
<p>以自动更新程序为例，在驱动程序页面完成更新即可</p>
<p><img src="/2023/07/11/pytorch%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230711105745926.png" alt="image-20230711105745926"></p>
<p>更新完成后，查看N卡设置</p>
<p><img src="/2023/07/11/pytorch%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230711105810959.png" alt="image-20230711105810959"></p>
<p><img src="/2023/07/11/pytorch%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230711105844840.png" alt="image-20230711105844840"></p>
<p>发现CUDA版本已经更新到12.2.19</p>
<h3 id="下载pytorch"><a href="#下载pytorch" class="headerlink" title="下载pytorch"></a>下载pytorch</h3><p>在安装了conda的情况下，在命令行输入(windows下powershell需要开管理员权限)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia</span><br></pre></td></tr></table></figure>
<p>在ipython中验证pytorch是否正确安装</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Python 3.10.9 | packaged by Anaconda, Inc. | (main, Mar  1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)] on win32</span><br><span class="line">Type <span class="string">&quot;help&quot;</span>, <span class="string">&quot;copyright&quot;</span>, <span class="string">&quot;credits&quot;</span> or <span class="string">&quot;license&quot;</span> <span class="keyword">for</span> more information.</span><br><span class="line">&gt;&gt;&gt; import torch</span><br><span class="line">&gt;&gt;&gt; x = torch.rand(5,3)</span><br><span class="line">&gt;&gt;&gt; <span class="built_in">print</span>(x)</span><br><span class="line">tensor([[0.4660, 0.0961, 0.7834],</span><br><span class="line">        [0.3614, 0.2262, 0.8813],</span><br><span class="line">        [0.9523, 0.8653, 0.2905],</span><br><span class="line">        [0.4628, 0.0707, 0.6660],</span><br><span class="line">        [0.4278, 0.7403, 0.2035]])</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>
<h3 id="为conda环境配置pytorch"><a href="#为conda环境配置pytorch" class="headerlink" title="为conda环境配置pytorch"></a>为conda环境配置pytorch</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda create -n MyEnv pytorch</span><br><span class="line">conda activate MyEnv</span><br></pre></td></tr></table></figure>
<p><img src="/2023/07/11/pytorch%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230711152214412.png" alt="image-20230711152214412"></p>
<p>使用jupyter notebook进行验证。</p>
<h2 id="pytorch的基本使用"><a href="#pytorch的基本使用" class="headerlink" title="pytorch的基本使用"></a>pytorch的基本使用</h2><p>详情请见<a herf="https://pytorch.org/docs/stable/torch.html">pytorch官方文档</a></p>
<h3 id="张量-Tensors"><a href="#张量-Tensors" class="headerlink" title="张量(Tensors)"></a>张量(Tensors)</h3><p>pytorch一大作用是替换numpy，使用张量（tensors)替代numpy的多维数组(ndarrays),前者的优势是可以部署到GPU上来加速向量运算。</p>
<p>必须使用的库是torch</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>
<h4 id="定义张量"><a href="#定义张量" class="headerlink" title="定义张量"></a>定义张量</h4><p>用法和numpy.rand,empty,zeros类似</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.rand() </span><br><span class="line">torch.empty() <span class="comment"># torch.new_empty()</span></span><br><span class="line">torch.zeros() <span class="comment"># torch.new_zeros()</span></span><br><span class="line">torch.ones() <span class="comment"># torch.new_ones()</span></span><br></pre></td></tr></table></figure>
<p>可以将python list,numpy ndarray转为tensor</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.tensor([[<span class="number">1.</span>, -<span class="number">1.</span>], [<span class="number">1.</span>, -<span class="number">1.</span>]])</span><br><span class="line">torch.tensor(np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]))</span><br></pre></td></tr></table></figure>
<p>利用原有张量形状创建张量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.randn_like(old_tensor,dtype=...)</span><br></pre></td></tr></table></figure>
<h4 id="张量运算"><a href="#张量运算" class="headerlink" title="张量运算"></a>张量运算</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">A = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">B = torch.tensor([[<span class="number">3</span>,<span class="number">5</span>],[<span class="number">9</span>,<span class="number">11</span>]])</span><br><span class="line"><span class="built_in">print</span>(A,B)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>​    tensor([[1, 2],<br>​            [3, 4]]) tensor([[ 3,  5],<br>​            [ 9, 11]])<br>​    </p>
<h4 id="张量运算-1"><a href="#张量运算-1" class="headerlink" title="张量运算"></a>张量运算</h4><h5 id="加法"><a href="#加法" class="headerlink" title="加法"></a>加法</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加法</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;user operator + &#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(A+B)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;user add(tensor1,tensor2,[out = tensor3])&#x27;</span>)</span><br><span class="line">C = torch.empty_like(A)</span><br><span class="line"><span class="built_in">print</span>(torch.add(A,B,out = C))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output C&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(C)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;A.add(B)&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(A.add(B))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;oprand after add&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(A,B)</span><br></pre></td></tr></table></figure>
<p>​    user operator +<br>​    tensor([[ 4,  7],<br>​            [12, 15]])<br>​    user add(tensor1,tensor2,[out = tensor3])<br>​    tensor([[ 4,  7],<br>​            [12, 15]])<br>​    output C<br>​    tensor([[ 4,  7],<br>​            [12, 15]])<br>​    A.add(B)<br>​    tensor([[ 4,  7],<br>​            [12, 15]])<br>​    oprand after add<br>​    tensor([[1, 2],<br>​            [3, 4]]) tensor([[ 3,  5],<br>​            [ 9, 11]])<br>​    </p>
<h4 id="cuda张量"><a href="#cuda张量" class="headerlink" title="cuda张量"></a>cuda张量</h4><p>tensor可以通过.to方法部署到不同的计算设备上</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span>)          <span class="comment"># 定义一个 CUDA 设备对象</span></span><br><span class="line">    y = torch.ones_like(A, device=device)  <span class="comment"># 显示创建在 GPU 上的一个 tensor</span></span><br><span class="line">    x = y.to(device)                       <span class="comment"># 也可以采用 .to(&quot;cuda&quot;) </span></span><br><span class="line">    z = x + y</span><br><span class="line">    <span class="built_in">print</span>(z)</span><br><span class="line">    <span class="built_in">print</span>(z.to(<span class="string">&quot;cpu&quot;</span>, torch.double))       <span class="comment"># .to() 方法也可以改变数值类型</span></span><br></pre></td></tr></table></figure>
<h3 id="梯度-grad"><a href="#梯度-grad" class="headerlink" title="梯度(grad)"></a>梯度(grad)</h3><p>梯度的概念是机器学习中的重中之重。</p>
<p>在pytorch中，pytorch中的autograd库提供了对tensor所有运算自动跟踪计算梯度的功能。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">A = torch.rand(<span class="number">3</span>,<span class="number">3</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(A)</span><br><span class="line">B = A + <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(B)</span><br><span class="line">C= B*B*<span class="number">3</span></span><br><span class="line"><span class="built_in">print</span>(C)</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<p><img src="/2023/07/11/pytorch%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230711162307175.png" alt="image-20230711162307175"></p>
<p>使用backward方法使用反向传播计算A张量的梯度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">out = C.mean()</span><br><span class="line"><span class="built_in">print</span>(out)</span><br><span class="line">out.backward()</span><br><span class="line"><span class="built_in">print</span>(A.grad)</span><br></pre></td></tr></table></figure>
<p><img src="/2023/07/11/pytorch%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230711162845636.png" alt="image-20230711162845636"></p>
]]></content>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>nn_tutorial</title>
    <url>/2023/07/13/nn-tutorial/</url>
    <content><![CDATA[<p>这篇文章是官网上的一篇jupyter notebook, 主要讲了pytorch学习的工作原理，以及如何一步步从手动训练模型到使用pytorch高度抽象化地训练模型。<br><span id="more"></span><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># For tips on running notebooks in Google Colab, see</span></span><br><span class="line"><span class="comment"># https://pytorch.org/tutorials/beginner/colab</span></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure></p>
<h1 id="What-is-torch-nn-really"><a href="#What-is-torch-nn-really" class="headerlink" title="What is torch.nn really?"></a>What is <code>torch.nn</code> <em>really</em>?</h1><p><strong>Authors:</strong> Jeremy Howard, <a href="https://www.fast.ai">fast.ai</a>. Thanks to Rachel Thomas and Francisco Ingham.</p>
<p>We recommend running this tutorial as a notebook, not a script. To download the notebook (<code>.ipynb</code>) file,<br>click the link at the top of the page.</p>
<p>PyTorch provides the elegantly designed modules and classes <a href="https://pytorch.org/docs/stable/nn.html">torch.nn</a> ,<br><a href="https://pytorch.org/docs/stable/optim.html">torch.optim</a> ,<br><a href="https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset">Dataset</a> ,<br>and <a href="https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader">DataLoader</a><br>to help you create and train neural networks.<br>In order to fully utilize their power and customize<br>them for your problem, you need to really understand exactly what they’re<br>doing. To develop this understanding, we will first train basic neural net<br>on the MNIST data set without using any features from these models; we will<br>initially only use the most basic PyTorch tensor functionality. Then, we will<br>incrementally add one feature from <code>torch.nn</code>, <code>torch.optim</code>, <code>Dataset</code>, or<br><code>DataLoader</code> at a time, showing exactly what each piece does, and how it<br>works to make the code either more concise, or more flexible.</p>
<p><strong>This tutorial assumes you already have PyTorch installed, and are familiar<br>with the basics of tensor operations.</strong> (If you’re familiar with Numpy array<br>operations, you’ll find the PyTorch tensor operations used here nearly identical).</p>
<h2 id="MNIST-data-setup"><a href="#MNIST-data-setup" class="headerlink" title="MNIST data setup"></a>MNIST data setup</h2><p>We will use the classic <a href="http://deeplearning.net/data/mnist/">MNIST</a> dataset,<br>which consists of black-and-white images of hand-drawn digits (between 0 and 9).</p>
<p>We will use <a href="https://docs.python.org/3/library/pathlib.html">pathlib</a><br>for dealing with paths (part of the Python 3 standard library), and will<br>download the dataset using<br><a href="http://docs.python-requests.org/en/master/">requests</a>. We will only<br>import modules when we use them, so you can see exactly what’s being<br>used at each point.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">DATA_PATH = Path(<span class="string">&quot;data&quot;</span>)</span><br><span class="line">PATH = DATA_PATH / <span class="string">&quot;mnist&quot;</span></span><br><span class="line"></span><br><span class="line">PATH.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">URL = <span class="string">&quot;https://github.com/pytorch/tutorials/raw/main/_static/&quot;</span></span><br><span class="line">FILENAME = <span class="string">&quot;mnist.pkl.gz&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> (PATH / FILENAME).exists():</span><br><span class="line">        content = requests.get(URL + FILENAME).content</span><br><span class="line">        (PATH / FILENAME).<span class="built_in">open</span>(<span class="string">&quot;wb&quot;</span>).write(content)</span><br></pre></td></tr></table></figure>
<p>This dataset is in numpy array format, and has been stored using pickle,<br>a python-specific format for serializing data.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> gzip.<span class="built_in">open</span>((PATH / FILENAME).as_posix(), <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=<span class="string">&quot;latin-1&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>Each image is 28 x 28, and is being stored as a flattened row of length<br>784 (=28x28). Let’s take a look at one; we need to reshape it to 2d<br>first.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">pyplot.imshow(x_train[<span class="number">0</span>].reshape((<span class="number">28</span>, <span class="number">28</span>)), cmap=<span class="string">&quot;gray&quot;</span>)</span><br><span class="line"><span class="comment"># ``pyplot.show()`` only if not on Colab</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">import</span> google.colab</span><br><span class="line"><span class="keyword">except</span> ImportError:</span><br><span class="line">    pyplot.show()</span><br><span class="line"><span class="built_in">print</span>(x_train.shape)</span><br></pre></td></tr></table></figure>
<p>PyTorch uses <code>torch.tensor</code>, rather than numpy arrays, so we need to<br>convert our data.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x_train, y_train, x_valid, y_valid = <span class="built_in">map</span>(</span><br><span class="line">    torch.tensor, (x_train, y_train, x_valid, y_valid)</span><br><span class="line">)</span><br><span class="line">n, c = x_train.shape</span><br><span class="line"><span class="built_in">print</span>(x_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(x_train.shape)</span><br><span class="line"><span class="built_in">print</span>(y_train.<span class="built_in">min</span>(), y_train.<span class="built_in">max</span>())</span><br></pre></td></tr></table></figure>
<h2 id="Neural-net-from-scratch-without-torch-nn"><a href="#Neural-net-from-scratch-without-torch-nn" class="headerlink" title="Neural net from scratch (without torch.nn)"></a>Neural net from scratch (without <code>torch.nn</code>)</h2><p>Let’s first create a model using nothing but PyTorch tensor operations. We’re assuming<br>you’re already familiar with the basics of neural networks. (If you’re not, you can<br>learn them at <a href="https://course.fast.ai">course.fast.ai</a>).</p>
<p>PyTorch provides methods to create random or zero-filled tensors, which we will<br>use to create our weights and bias for a simple linear model. These are just regular<br>tensors, with one very special addition: we tell PyTorch that they require a<br>gradient. This causes PyTorch to record all of the operations done on the tensor,<br>so that it can calculate the gradient during back-propagation <em>automatically</em>!</p>
<p>For the weights, we set <code>requires_grad</code> <strong>after</strong> the initialization, since we<br>don’t want that step included in the gradient. (Note that a trailing <code>_</code> in<br>PyTorch signifies that the operation is performed in-place.)</p>
<div class="alert alert-info"><h4>Note</h4><p>We are initializing the weights here with
   [Xavier initialisation](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
   (by multiplying with ``1/sqrt(n)``).</p></div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">weights = torch.randn(<span class="number">784</span>, <span class="number">10</span>) / math.sqrt(<span class="number">784</span>)</span><br><span class="line">weights.requires_grad_()</span><br><span class="line">bias = torch.zeros(<span class="number">10</span>, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>Thanks to PyTorch’s ability to calculate gradients automatically, we can<br>use any standard Python function (or callable object) as a model! So<br>let’s just write a plain matrix multiplication and broadcasted addition<br>to create a simple linear model. We also need an activation function, so<br>we’ll write <code>log_softmax</code> and use it. Remember: although PyTorch<br>provides lots of prewritten loss functions, activation functions, and<br>so forth, you can easily write your own using plain python. PyTorch will<br>even create fast GPU or vectorized CPU code for your function<br>automatically.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">log_softmax</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x - x.exp().<span class="built_in">sum</span>(-<span class="number">1</span>).log().unsqueeze(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model</span>(<span class="params">xb</span>):</span><br><span class="line">    <span class="keyword">return</span> log_softmax(xb @ weights + bias)</span><br></pre></td></tr></table></figure>
<p>In the above, the <code>@</code> stands for the matrix multiplication operation. We will call<br>our function on one batch of data (in this case, 64 images).  This is<br>one <em>forward pass</em>.  Note that our predictions won’t be any better than<br>random at this stage, since we start with random weights.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bs = <span class="number">64</span>  <span class="comment"># batch size</span></span><br><span class="line"></span><br><span class="line">xb = x_train[<span class="number">0</span>:bs]  <span class="comment"># a mini-batch from x</span></span><br><span class="line">preds = model(xb)  <span class="comment"># predictions</span></span><br><span class="line">preds[<span class="number">0</span>], preds.shape</span><br><span class="line"><span class="built_in">print</span>(preds[<span class="number">0</span>], preds.shape)</span><br></pre></td></tr></table></figure>
<p>As you see, the <code>preds</code> tensor contains not only the tensor values, but also a<br>gradient function. We’ll use this later to do backprop.</p>
<p>Let’s implement negative log-likelihood to use as the loss function<br>(again, we can just use standard Python):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">nll</span>(<span class="params"><span class="built_in">input</span>, target</span>):</span><br><span class="line">    <span class="keyword">return</span> -<span class="built_in">input</span>[<span class="built_in">range</span>(target.shape[<span class="number">0</span>]), target].mean()</span><br><span class="line"></span><br><span class="line">loss_func = nll</span><br></pre></td></tr></table></figure>
<p>Let’s check our loss with our random model, so we can see if we improve<br>after a backprop pass later.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">yb = y_train[<span class="number">0</span>:bs]</span><br><span class="line"><span class="built_in">print</span>(loss_func(preds, yb))</span><br></pre></td></tr></table></figure>
<p>Let’s also implement a function to calculate the accuracy of our model.<br>For each prediction, if the index with the largest value matches the<br>target value, then the prediction was correct.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">out, yb</span>):</span><br><span class="line">    preds = torch.argmax(out, dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (preds == yb).<span class="built_in">float</span>().mean()</span><br></pre></td></tr></table></figure>
<p>Let’s check the accuracy of our random model, so we can see if our<br>accuracy improves as our loss improves.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(accuracy(preds, yb))</span><br></pre></td></tr></table></figure>
<p>We can now run a training loop.  For each iteration, we will:</p>
<ul>
<li>select a mini-batch of data (of size <code>bs</code>)</li>
<li>use the model to make predictions</li>
<li>calculate the loss</li>
<li><code>loss.backward()</code> updates the gradients of the model, in this case, <code>weights</code><br>and <code>bias</code>.</li>
</ul>
<p>We now use these gradients to update the weights and bias.  We do this<br>within the <code>torch.no_grad()</code> context manager, because we do not want these<br>actions to be recorded for our next calculation of the gradient.  You can read<br>more about how PyTorch’s Autograd records operations<br><a href="https://pytorch.org/docs/stable/notes/autograd.html">here</a>.</p>
<p>We then set the<br>gradients to zero, so that we are ready for the next loop.<br>Otherwise, our gradients would record a running tally of all the operations<br>that had happened (i.e. <code>loss.backward()</code> <em>adds</em> the gradients to whatever is<br>already stored, rather than replacing them).</p>
<p>.. tip:: You can use the standard python debugger to step through PyTorch<br>   code, allowing you to check the various variable values at each step.<br>   Uncomment <code>set_trace()</code> below to try it out.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython.core.debugger <span class="keyword">import</span> set_trace</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.5</span>  <span class="comment"># learning rate</span></span><br><span class="line">epochs = <span class="number">2</span>  <span class="comment"># how many epochs to train for</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">        <span class="comment">#         set_trace()</span></span><br><span class="line">        start_i = i * bs</span><br><span class="line">        end_i = start_i + bs</span><br><span class="line">        xb = x_train[start_i:end_i]</span><br><span class="line">        yb = y_train[start_i:end_i]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            weights -= weights.grad * lr</span><br><span class="line">            bias -= bias.grad * lr</span><br><span class="line">            weights.grad.zero_()</span><br><span class="line">            bias.grad.zero_()</span><br></pre></td></tr></table></figure>
<p>That’s it: we’ve created and trained a minimal neural network (in this case, a<br>logistic regression, since we have no hidden layers) entirely from scratch!</p>
<p>Let’s check the loss and accuracy and compare those to what we got<br>earlier. We expect that the loss will have decreased and accuracy to<br>have increased, and they have.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb), accuracy(model(xb), yb))</span><br></pre></td></tr></table></figure>
<h2 id="Using-torch-nn-functional"><a href="#Using-torch-nn-functional" class="headerlink" title="Using torch.nn.functional"></a>Using <code>torch.nn.functional</code></h2><p>We will now refactor our code, so that it does the same thing as before, only<br>we’ll start taking advantage of PyTorch’s <code>nn</code> classes to make it more concise<br>and flexible. At each step from here, we should be making our code one or more<br>of: shorter, more understandable, and/or more flexible.</p>
<p>The first and easiest step is to make our code shorter by replacing our<br>hand-written activation and loss functions with those from <code>torch.nn.functional</code><br>(which is generally imported into the namespace <code>F</code> by convention). This module<br>contains all the functions in the <code>torch.nn</code> library (whereas other parts of the<br>library contain classes). As well as a wide range of loss and activation<br>functions, you’ll also find here some convenient functions for creating neural<br>nets, such as pooling functions. (There are also functions for doing convolutions,<br>linear layers, etc, but as we’ll see, these are usually better handled using<br>other parts of the library.)</p>
<p>If you’re using negative log likelihood loss and log softmax activation,<br>then Pytorch provides a single function <code>F.cross_entropy</code> that combines<br>the two. So we can even remove the activation function from our model.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">loss_func = F.cross_entropy</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model</span>(<span class="params">xb</span>):</span><br><span class="line">    <span class="keyword">return</span> xb @ weights + bias</span><br></pre></td></tr></table></figure>
<p>Note that we no longer call <code>log_softmax</code> in the <code>model</code> function. Let’s<br>confirm that our loss and accuracy are the same as before:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb), accuracy(model(xb), yb))</span><br></pre></td></tr></table></figure>
<h2 id="Refactor-using-nn-Module"><a href="#Refactor-using-nn-Module" class="headerlink" title="Refactor using nn.Module"></a>Refactor using <code>nn.Module</code></h2><p>Next up, we’ll use <code>nn.Module</code> and <code>nn.Parameter</code>, for a clearer and more<br>concise training loop. We subclass <code>nn.Module</code> (which itself is a class and<br>able to keep track of state).  In this case, we want to create a class that<br>holds our weights, bias, and method for the forward step.  <code>nn.Module</code> has a<br>number of attributes and methods (such as <code>.parameters()</code> and <code>.zero_grad()</code>)<br>which we will be using.</p>
<div class="alert alert-info"><h4>Note</h4><p>``nn.Module`` (uppercase M) is a PyTorch specific concept, and is a
   class we'll be using a lot. ``nn.Module`` is not to be confused with the Python
   concept of a (lowercase ``m``) [module](https://docs.python.org/3/tutorial/modules.html),
   which is a file of Python code that can be imported.</p></div>




<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Mnist_Logistic</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weights = nn.Parameter(torch.randn(<span class="number">784</span>, <span class="number">10</span>) / math.sqrt(<span class="number">784</span>))</span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, xb</span>):</span><br><span class="line">        <span class="keyword">return</span> xb @ self.weights + self.bias</span><br></pre></td></tr></table></figure>
<p>Since we’re now using an object instead of just using a function, we<br>first have to instantiate our model:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Mnist_Logistic()</span><br></pre></td></tr></table></figure>
<p>Now we can calculate the loss in the same way as before. Note that<br><code>nn.Module</code> objects are used as if they are functions (i.e they are<br><em>callable</em>), but behind the scenes Pytorch will call our <code>forward</code><br>method automatically.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<p>Previously for our training loop we had to update the values for each parameter<br>by name, and manually zero out the grads for each parameter separately, like this:</p>
<p>::</p>
<p>   with torch.no<em>grad():<br>       weights -= weights.grad <em> lr<br>       bias -= bias.grad </em> lr<br>       weights.grad.zero</em>()<br>       bias.grad.zero_()</p>
<p>Now we can take advantage of model.parameters() and model.zero_grad() (which<br>are both defined by PyTorch for <code>nn.Module</code>) to make those steps more concise<br>and less prone to the error of forgetting some of our parameters, particularly<br>if we had a more complicated model:</p>
<p>::</p>
<p>   with torch.no_grad():<br>       for p in model.parameters(): p -= p.grad * lr<br>       model.zero_grad()</p>
<p>We’ll wrap our little training loop in a <code>fit</code> function so we can run it<br>again later.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>():</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">            start_i = i * bs</span><br><span class="line">            end_i = start_i + bs</span><br><span class="line">            xb = x_train[start_i:end_i]</span><br><span class="line">            yb = y_train[start_i:end_i]</span><br><span class="line">            pred = model(xb)</span><br><span class="line">            loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">                    p -= p.grad * lr</span><br><span class="line">                model.zero_grad()</span><br><span class="line"></span><br><span class="line">fit()</span><br></pre></td></tr></table></figure>
<p>Let’s double-check that our loss has gone down:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<h2 id="Refactor-using-nn-Linear"><a href="#Refactor-using-nn-Linear" class="headerlink" title="Refactor using nn.Linear"></a>Refactor using <code>nn.Linear</code></h2><p>We continue to refactor our code.  Instead of manually defining and<br>initializing <code>self.weights</code> and <code>self.bias</code>, and calculating <code>xb  @
self.weights + self.bias</code>, we will instead use the Pytorch class<br><a href="https://pytorch.org/docs/stable/nn.html#linear-layers">nn.Linear</a> for a<br>linear layer, which does all that for us. Pytorch has many types of<br>predefined layers that can greatly simplify our code, and often makes it<br>faster too.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Mnist_Logistic</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.lin = nn.Linear(<span class="number">784</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, xb</span>):</span><br><span class="line">        <span class="keyword">return</span> self.lin(xb)</span><br></pre></td></tr></table></figure>
<p>We instantiate our model and calculate the loss in the same way as before:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Mnist_Logistic()</span><br><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<p>We are still able to use our same <code>fit</code> method as before.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<h2 id="Refactor-using-torch-optim"><a href="#Refactor-using-torch-optim" class="headerlink" title="Refactor using torch.optim"></a>Refactor using <code>torch.optim</code></h2><p>Pytorch also has a package with various optimization algorithms, <code>torch.optim</code>.<br>We can use the <code>step</code> method from our optimizer to take a forward step, instead<br>of manually updating each parameter.</p>
<p>This will let us replace our previous manually coded optimization step:</p>
<p>::</p>
<p>   with torch.no_grad():<br>       for p in model.parameters(): p -= p.grad * lr<br>       model.zero_grad()</p>
<p>and instead use just:</p>
<p>::</p>
<p>   opt.step()<br>   opt.zero_grad()</p>
<p>(<code>optim.zero_grad()</code> resets the gradient to 0 and we need to call it before<br>computing the gradient for the next minibatch.)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br></pre></td></tr></table></figure>
<p>We’ll define a little function to create our model and optimizer so we<br>can reuse it in the future.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_model</span>():</span><br><span class="line">    model = Mnist_Logistic()</span><br><span class="line">    <span class="keyword">return</span> model, optim.SGD(model.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">model, opt = get_model()</span><br><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">        start_i = i * bs</span><br><span class="line">        end_i = start_i + bs</span><br><span class="line">        xb = x_train[start_i:end_i]</span><br><span class="line">        yb = y_train[start_i:end_i]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<h2 id="Refactor-using-Dataset"><a href="#Refactor-using-Dataset" class="headerlink" title="Refactor using Dataset"></a>Refactor using Dataset</h2><p>PyTorch has an abstract Dataset class.  A Dataset can be anything that has<br>a <code>__len__</code> function (called by Python’s standard <code>len</code> function) and<br>a <code>__getitem__</code> function as a way of indexing into it.<br><a href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html">This tutorial</a><br>walks through a nice example of creating a custom <code>FacialLandmarkDataset</code> class<br>as a subclass of <code>Dataset</code>.</p>
<p>PyTorch’s <a href="https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset">TensorDataset</a><br>is a Dataset wrapping tensors. By defining a length and way of indexing,<br>this also gives us a way to iterate, index, and slice along the first<br>dimension of a tensor. This will make it easier to access both the<br>independent and dependent variables in the same line as we train.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset</span><br></pre></td></tr></table></figure>
<p>Both <code>x_train</code> and <code>y_train</code> can be combined in a single <code>TensorDataset</code>,<br>which will be easier to iterate over and slice.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br></pre></td></tr></table></figure>
<p>Previously, we had to iterate through minibatches of <code>x</code> and <code>y</code> values separately:</p>
<p>::</p>
<p>   xb = x_train[start_i:end_i]<br>   yb = y_train[start_i:end_i]</p>
<p>Now, we can do these two steps together:</p>
<p>::</p>
<p>   xb,yb = train_ds[i<em>bs : i</em>bs+bs]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>((n - <span class="number">1</span>) // bs + <span class="number">1</span>):</span><br><span class="line">        xb, yb = train_ds[i * bs: i * bs + bs]</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<h2 id="Refactor-using-DataLoader"><a href="#Refactor-using-DataLoader" class="headerlink" title="Refactor using DataLoader"></a>Refactor using <code>DataLoader</code></h2><p>PyTorch’s <code>DataLoader</code> is responsible for managing batches. You can<br>create a <code>DataLoader</code> from any <code>Dataset</code>. <code>DataLoader</code> makes it easier<br>to iterate over batches. Rather than having to use <code>train_ds[i*bs : i*bs+bs]</code>,<br>the <code>DataLoader</code> gives us each minibatch automatically.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br><span class="line">train_dl = DataLoader(train_ds, batch_size=bs)</span><br></pre></td></tr></table></figure>
<p>Previously, our loop iterated over batches <code>(xb, yb)</code> like this:</p>
<p>::</p>
<p>   for i in range((n-1)//bs + 1):<br>       xb,yb = train_ds[i<em>bs : i</em>bs+bs]<br>       pred = model(xb)</p>
<p>Now, our loop is much cleaner, as <code>(xb, yb)</code> are loaded automatically from the data loader:</p>
<p>::</p>
<p>   for xb,yb in train_dl:<br>       pred = model(xb)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(loss_func(model(xb), yb))</span><br></pre></td></tr></table></figure>
<p>Thanks to PyTorch’s <code>nn.Module</code>, <code>nn.Parameter</code>, <code>Dataset</code>, and <code>DataLoader</code>,<br>our training loop is now dramatically smaller and easier to understand. Let’s<br>now try to add the basic features necessary to create effective models in practice.</p>
<h2 id="Add-validation"><a href="#Add-validation" class="headerlink" title="Add validation"></a>Add validation</h2><p>In section 1, we were just trying to get a reasonable training loop set up for<br>use on our training data.  In reality, you <strong>always</strong> should also have<br>a <a href="https://www.fast.ai/2017/11/13/validation-sets/">validation set</a>, in order<br>to identify if you are overfitting.</p>
<p>Shuffling the training data is<br><a href="https://www.quora.com/Does-the-order-of-training-data-matter-when-training-neural-networks">important</a><br>to prevent correlation between batches and overfitting. On the other hand, the<br>validation loss will be identical whether we shuffle the validation set or not.<br>Since shuffling takes extra time, it makes no sense to shuffle the validation data.</p>
<p>We’ll use a batch size for the validation set that is twice as large as<br>that for the training set. This is because the validation set does not<br>need backpropagation and thus takes less memory (it doesn’t need to<br>store the gradients). We take advantage of this to use a larger batch<br>size and compute the loss more quickly.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br><span class="line">train_dl = DataLoader(train_ds, batch_size=bs, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">valid_ds = TensorDataset(x_valid, y_valid)</span><br><span class="line">valid_dl = DataLoader(valid_ds, batch_size=bs * <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>We will calculate and print the validation loss at the end of each epoch.</p>
<p>(Note that we always call <code>model.train()</code> before training, and <code>model.eval()</code><br>before inference, because these are used by layers such as <code>nn.BatchNorm2d</code><br>and <code>nn.Dropout</code> to ensure appropriate behavior for these different phases.)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model, opt = get_model()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">        pred = model(xb)</span><br><span class="line">        loss = loss_func(pred, yb)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        valid_loss = <span class="built_in">sum</span>(loss_func(model(xb), yb) <span class="keyword">for</span> xb, yb <span class="keyword">in</span> valid_dl)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(epoch, valid_loss / <span class="built_in">len</span>(valid_dl))</span><br></pre></td></tr></table></figure>
<h2 id="Create-fit-and-get-data"><a href="#Create-fit-and-get-data" class="headerlink" title="Create fit() and get_data()"></a>Create fit() and get_data()</h2><p>We’ll now do a little refactoring of our own. Since we go through a similar<br>process twice of calculating the loss for both the training set and the<br>validation set, let’s make that into its own function, <code>loss_batch</code>, which<br>computes the loss for one batch.</p>
<p>We pass an optimizer in for the training set, and use it to perform<br>backprop.  For the validation set, we don’t pass an optimizer, so the<br>method doesn’t perform backprop.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">loss_batch</span>(<span class="params">model, loss_func, xb, yb, opt=<span class="literal">None</span></span>):</span><br><span class="line">    loss = loss_func(model(xb), yb)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss.item(), <span class="built_in">len</span>(xb)</span><br></pre></td></tr></table></figure>
<p><code>fit</code> runs the necessary operations to train our model and compute the<br>training and validation losses for each epoch.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">epochs, model, loss_func, opt, train_dl, valid_dl</span>):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">for</span> xb, yb <span class="keyword">in</span> train_dl:</span><br><span class="line">            loss_batch(model, loss_func, xb, yb, opt)</span><br><span class="line"></span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            losses, nums = <span class="built_in">zip</span>(</span><br><span class="line">                *[loss_batch(model, loss_func, xb, yb) <span class="keyword">for</span> xb, yb <span class="keyword">in</span> valid_dl]</span><br><span class="line">            )</span><br><span class="line">        val_loss = np.<span class="built_in">sum</span>(np.multiply(losses, nums)) / np.<span class="built_in">sum</span>(nums)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(epoch, val_loss)</span><br></pre></td></tr></table></figure>
<p><code>get_data</code> returns dataloaders for the training and validation sets.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_data</span>(<span class="params">train_ds, valid_ds, bs</span>):</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        DataLoader(train_ds, batch_size=bs, shuffle=<span class="literal">True</span>),</span><br><span class="line">        DataLoader(valid_ds, batch_size=bs * <span class="number">2</span>),</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>Now, our whole process of obtaining the data loaders and fitting the<br>model can be run in 3 lines of code:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_dl, valid_dl = get_data(train_ds, valid_ds, bs)</span><br><span class="line">model, opt = get_model()</span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>
<p>You can use these basic 3 lines of code to train a wide variety of models.<br>Let’s see if we can use them to train a convolutional neural network (CNN)!</p>
<h2 id="Switch-to-CNN"><a href="#Switch-to-CNN" class="headerlink" title="Switch to CNN"></a>Switch to CNN</h2><p>We are now going to build our neural network with three convolutional layers.<br>Because none of the functions in the previous section assume anything about<br>the model form, we’ll be able to use them to train a CNN without any modification.</p>
<p>We will use PyTorch’s predefined<br><a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d">Conv2d</a> class<br>as our convolutional layer. We define a CNN with 3 convolutional layers.<br>Each convolution is followed by a ReLU.  At the end, we perform an<br>average pooling.  (Note that <code>view</code> is PyTorch’s version of Numpy’s<br><code>reshape</code>)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Mnist_CNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, xb</span>):</span><br><span class="line">        xb = xb.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">        xb = F.relu(self.conv1(xb))</span><br><span class="line">        xb = F.relu(self.conv2(xb))</span><br><span class="line">        xb = F.relu(self.conv3(xb))</span><br><span class="line">        xb = F.avg_pool2d(xb, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">return</span> xb.view(-<span class="number">1</span>, xb.size(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.1</span></span><br></pre></td></tr></table></figure>
<p><a href="https://cs231n.github.io/neural-networks-3/#sgd">Momentum</a> is a variation on<br>stochastic gradient descent that takes previous updates into account as well<br>and generally leads to faster training.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Mnist_CNN()</span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>
<h2 id="Using-nn-Sequential"><a href="#Using-nn-Sequential" class="headerlink" title="Using nn.Sequential"></a>Using <code>nn.Sequential</code></h2><p><code>torch.nn</code> has another handy class we can use to simplify our code:<br><a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential">Sequential</a> .<br>A <code>Sequential</code> object runs each of the modules contained within it, in a<br>sequential manner. This is a simpler way of writing our neural network.</p>
<p>To take advantage of this, we need to be able to easily define a<br><strong>custom layer</strong> from a given function.  For instance, PyTorch doesn’t<br>have a <code>view</code> layer, and we need to create one for our network. <code>Lambda</code><br>will create a layer that we can then use when defining a network with<br><code>Sequential</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Lambda</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, func</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.func = func</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.func(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br></pre></td></tr></table></figure>
<p>The model created with <code>Sequential</code> is simple:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">    Lambda(preprocess),</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.AvgPool2d(<span class="number">4</span>),</span><br><span class="line">    Lambda(<span class="keyword">lambda</span> x: x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>
<h2 id="Wrapping-DataLoader"><a href="#Wrapping-DataLoader" class="headerlink" title="Wrapping DataLoader"></a>Wrapping <code>DataLoader</code></h2><p>Our CNN is fairly concise, but it only works with MNIST, because:</p>
<ul>
<li>It assumes the input is a 28*28 long vector</li>
<li>It assumes that the final CNN grid size is 4*4 (since that’s the average pooling kernel size we used)</li>
</ul>
<p>Let’s get rid of these two assumptions, so our model works with any 2d<br>single channel image. First, we can remove the initial Lambda layer by<br>moving the data preprocessing into a generator:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span> x.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WrappedDataLoader</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dl, func</span>):</span><br><span class="line">        self.dl = dl</span><br><span class="line">        self.func = func</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.dl)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> b <span class="keyword">in</span> self.dl:</span><br><span class="line">            <span class="keyword">yield</span> (self.func(*b))</span><br><span class="line"></span><br><span class="line">train_dl, valid_dl = get_data(train_ds, valid_ds, bs)</span><br><span class="line">train_dl = WrappedDataLoader(train_dl, preprocess)</span><br><span class="line">valid_dl = WrappedDataLoader(valid_dl, preprocess)</span><br></pre></td></tr></table></figure>
<p>Next, we can replace <code>nn.AvgPool2d</code> with <code>nn.AdaptiveAvgPool2d</code>, which<br>allows us to define the size of the <em>output</em> tensor we want, rather than<br>the <em>input</em> tensor we have. As a result, our model will work with any<br>size input.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">16</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.AdaptiveAvgPool2d(<span class="number">1</span>),</span><br><span class="line">    Lambda(<span class="keyword">lambda</span> x: x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<p>Let’s try it out:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>
<h2 id="Using-your-GPU"><a href="#Using-your-GPU" class="headerlink" title="Using your GPU"></a>Using your GPU</h2><p>If you’re lucky enough to have access to a CUDA-capable GPU (you can<br>rent one for about $0.50/hour from most cloud providers) you can<br>use it to speed up your code. First check that your GPU is working in<br>Pytorch:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())</span><br></pre></td></tr></table></figure>
<p>And then create a device object for it:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dev = torch.device(</span><br><span class="line">    <span class="string">&quot;cuda&quot;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>Let’s update <code>preprocess</code> to move batches to the GPU:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span> x.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>).to(dev), y.to(dev)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_dl, valid_dl = get_data(train_ds, valid_ds, bs)</span><br><span class="line">train_dl = WrappedDataLoader(train_dl, preprocess)</span><br><span class="line">valid_dl = WrappedDataLoader(valid_dl, preprocess)</span><br></pre></td></tr></table></figure>
<p>Finally, we can move our model to the GPU.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.to(dev)</span><br><span class="line">opt = optim.SGD(model.parameters(), lr=lr, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<p>You should find it runs faster now:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>
<h2 id="Closing-thoughts"><a href="#Closing-thoughts" class="headerlink" title="Closing thoughts"></a>Closing thoughts</h2><p>We now have a general data pipeline and training loop which you can use for<br>training many types of models using Pytorch. To see how simple training a model<br>can now be, take a look at the <a href="https://github.com/fastai/fastai_dev/blob/master/dev_nb/mnist_sample.ipynb">mnist_sample notebook</a>_.</p>
<p>Of course, there are many things you’ll want to add, such as data augmentation,<br>hyperparameter tuning, monitoring training, transfer learning, and so forth.<br>These features are available in the fastai library, which has been developed<br>using the same design approach shown in this tutorial, providing a natural<br>next step for practitioners looking to take their models further.</p>
<p>We promised at the start of this tutorial we’d explain through example each of<br><code>torch.nn</code>, <code>torch.optim</code>, <code>Dataset</code>, and <code>DataLoader</code>. So let’s summarize<br>what we’ve seen:</p>
<ul>
<li><p><code>torch.nn</code>:</p>
<ul>
<li><code>Module</code>: creates a callable which behaves like a function, but can also<br>contain state(such as neural net layer weights). It knows what <code>Parameter</code> (s) it<br>contains and can zero all their gradients, loop through them for weight updates, etc.</li>
<li><code>Parameter</code>: a wrapper for a tensor that tells a <code>Module</code> that it has weights<br>that need updating during backprop. Only tensors with the <code>requires_grad</code> attribute set are updated</li>
<li><code>functional</code>: a module(usually imported into the <code>F</code> namespace by convention)<br>which contains activation functions, loss functions, etc, as well as non-stateful<br>versions of layers such as convolutional and linear layers.</li>
</ul>
</li>
<li><code>torch.optim</code>: Contains optimizers such as <code>SGD</code>, which update the weights<br>of <code>Parameter</code> during the backward step</li>
<li><code>Dataset</code>: An abstract interface of objects with a <code>__len__</code> and a <code>__getitem__</code>,<br>including classes provided with Pytorch such as <code>TensorDataset</code></li>
<li><code>DataLoader</code>: Takes any <code>Dataset</code> and creates an iterator which returns batches of data.</li>
</ul>
]]></content>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>resnet18在cifar10数据集上训练</title>
    <url>/2023/07/15/resnet18%E5%9C%A8cifar10%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E8%AE%AD%E7%BB%83/</url>
    <content><![CDATA[<p>这篇文章是我入门pytorch的第一个实验，实验torchvision中的resnet18在CIFAR数据集上进行训练<br><span id="more"></span></p>
<h1 id="使用ResNet18在CIFAR10上训练"><a href="#使用ResNet18在CIFAR10上训练" class="headerlink" title="使用ResNet18在CIFAR10上训练"></a>使用ResNet18在CIFAR10上训练</h1><h2 id="处理数据集"><a href="#处理数据集" class="headerlink" title="处理数据集"></a>处理数据集</h2><p>这里先处理data_batch_1中的一万个数据<br>按照官网中的说明使用unpickle函数进行读取,然后使用TensorDataset进行封装。<br>另外这里有一个细节，由于数据集中我们的训练数据是五万张图像，如果读取出来的五万张图像的List直接转为张量，那么速度会很慢，我们先使用numpy.array()将ndarray的list转化为single ndarray，转为张量的过程就会显著加速。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">PATH = <span class="string">&#x27;ianafp/CIFAR10/cifar-10-batches-py/data_batch_&#x27;</span></span><br><span class="line">TEST_BATCH = <span class="string">&#x27;ianafp/CIFAR10/cifar-10-batches-py/test_batch&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">unpickle</span>(<span class="params">file</span>):</span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> fo:</span><br><span class="line">        <span class="built_in">dict</span> = pickle.load(fo, encoding=<span class="string">&#x27;bytes&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span></span><br><span class="line">x_train,y_train = [],[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">6</span>):</span><br><span class="line">    dataset = unpickle(PATH+<span class="string">&#x27;&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">    <span class="comment"># print(dataset.keys())</span></span><br><span class="line">    x_train.extend(dataset[<span class="string">b&#x27;data&#x27;</span>])</span><br><span class="line">    y_train.extend(dataset[<span class="string">b&#x27;labels&#x27;</span>])</span><br><span class="line">dataset = unpickle(TEST_BATCH)</span><br><span class="line">x_valid,y_valid = dataset[<span class="string">b&#x27;data&#x27;</span>],dataset[<span class="string">b&#x27;labels&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>测试数据有效性</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(x_train.__len__())</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line">img = np.reshape(x_valid[<span class="number">8000</span>],(<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line">img = np.transpose(img,(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>))</span><br><span class="line">pyplot.imshow(img)</span><br></pre></td></tr></table></figure>
<p>将数据使用dataset和dataloader类进行封装</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset,DataLoader</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x_train.__len__())</span><br><span class="line">batch_size = <span class="number">256</span> </span><br><span class="line">x_train = np.array(x_train)</span><br><span class="line">y_train = np.array(y_train)</span><br><span class="line">x_valid = np.array(x_valid)</span><br><span class="line">y_valid = np.array(y_valid)</span><br><span class="line">x_train = np.reshape(x_train,(x_train.__len__(),<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line">x_valid = np.reshape(x_valid,(x_valid.__len__(),<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line">train_ds = TensorDataset(torch.tensor(x_train,dtype=torch.<span class="built_in">float</span>),torch.tensor(y_train,dtype=torch.<span class="built_in">float</span>))</span><br><span class="line">train_dl = DataLoader(train_ds,batch_size=batch_size)</span><br><span class="line">valid_ds = TensorDataset(torch.tensor(x_valid,dtype=torch.<span class="built_in">float</span>),torch.tensor(y_valid,dtype=torch.<span class="built_in">float</span>))</span><br><span class="line">valid_dl = DataLoader(valid_ds,batch_size=batch_size*<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h2 id="使用torchvison中的ResNet18"><a href="#使用torchvison中的ResNet18" class="headerlink" title="使用torchvison中的ResNet18"></a>使用torchvison中的ResNet18</h2><p>这里我们使用torchvision中的ResNet model</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> resnet18,ResNet18_Weights</span><br><span class="line">model = resnet18()</span><br></pre></td></tr></table></figure>
<p>设置损失函数，这里我们使用交叉熵损失函数(croos entropy error)</p>
<script type="math/tex; mode=display">
E = - \sum_k t_k \log y_k</script><p>其中，$t_k$为label,$y_k$为网络输出值<br>这里实现使用torch.nn.functional中封装好的交叉熵函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">loss_func = F.cross_entropy</span><br></pre></td></tr></table></figure>
<p>实现一个准确率函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">out, yb</span>):</span><br><span class="line">    preds = torch.argmax(out, dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (preds == yb).<span class="built_in">float</span>().mean()</span><br></pre></td></tr></table></figure>
<p>在训练前检验我们模型的损失函数值和准确率。<br>这里面要注意的点是pytorch卷积层是以浮点数工作的，而我们从图像中读取的训练数据集是byte类型的，因而我们要对tensor作类型转化。<br>另外torch.nn.functional中的交叉熵损失函数F.cross_entropy中标签以long类型工作，因而也需要作类型转化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x,y = train_ds[<span class="number">0</span>:batch_size]</span><br><span class="line">pred = model(x)</span><br><span class="line"><span class="built_in">print</span>(pred.dtype,y.dtype)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;loss_func = &#x27;</span>,loss_func(pred,y.long()))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;accuracy = &#x27;</span>,accuracy(pred,y))</span><br></pre></td></tr></table></figure>
<p>使用torch.optim进行梯度下降优化</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line">learning_rate = <span class="number">0.5</span></span><br><span class="line">opt = optim.SGD(model.parameters(),lr=learning_rate)</span><br><span class="line">epochs = <span class="number">8</span></span><br></pre></td></tr></table></figure>
<p>定义随机梯度下降函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">loss_batch</span>(<span class="params">model,loss_func,xb,yb,opt=<span class="literal">None</span></span>):</span><br><span class="line">    loss = loss_func(model(xb),yb.long())</span><br><span class="line">    <span class="keyword">if</span> opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line">    <span class="keyword">return</span> loss.item(),<span class="built_in">len</span>(xb)</span><br></pre></td></tr></table></figure>
<p>定义fit函数，fit函数中完成训练过程</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># def fit(epochs,model,loss_func,opt,train_dl,valid_dl):</span></span><br><span class="line"><span class="comment">#     for epoch in range(epochs):</span></span><br><span class="line"><span class="comment">#         model.train()</span></span><br><span class="line"><span class="comment">#         for xb,yb in train_dl:</span></span><br><span class="line"><span class="comment">#             loss_batch(model,loss_func,xb,yb.long(),opt)</span></span><br><span class="line"><span class="comment">#         model.eval()</span></span><br><span class="line"><span class="comment">#         with torch.no_grad():</span></span><br><span class="line"><span class="comment">#             losses,nums = zip(*[loss_batch(model,loss_func,xb,yb) for xb,yb in valid_dl])</span></span><br><span class="line"><span class="comment">#         val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)</span></span><br><span class="line"><span class="comment">#         print(epoch,val_loss)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">epochs,model,loss_func,opt,train_dl,valid_dl</span>):</span><br><span class="line">    epoch = <span class="number">0</span></span><br><span class="line">    pre_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">for</span> xb,yb <span class="keyword">in</span> train_dl:</span><br><span class="line">            loss_batch(model,loss_func,xb,yb.long(),opt)</span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            losses,nums = <span class="built_in">zip</span>(*[loss_batch(model,loss_func,xb,yb) <span class="keyword">for</span> xb,yb <span class="keyword">in</span> valid_dl])</span><br><span class="line">        val_loss = np.<span class="built_in">sum</span>(np.multiply(losses,nums)) / np.<span class="built_in">sum</span>(nums)</span><br><span class="line">        <span class="built_in">print</span>(epoch,val_loss)</span><br><span class="line">        <span class="keyword">import</span> math </span><br><span class="line">        <span class="keyword">if</span> math.fabs(val_loss-pre_loss)&lt;<span class="number">1e-9</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        pre_loss = val_loss</span><br><span class="line">        epoch = epoch + <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>试运行训练函数fit</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># fit(epochs,model,loss_func,opt,train_dl,valid_dl)</span></span><br></pre></td></tr></table></figure>
<p>运行时间过长。</p>
<h2 id="将模型部署在GPU上"><a href="#将模型部署在GPU上" class="headerlink" title="将模型部署在GPU上"></a>将模型部署在GPU上</h2><p>检验cuda是否可用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())</span><br><span class="line">dev = torch.device(<span class="string">&#x27;cuda&#x27;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>将数据集迁移到GPU<br>这里使用的技巧是定义了WrappedDataLoader类，该类是可迭代的，因为定义了<strong>iter</strong>方法。<br>又因为<strong>iter</strong>方法中使用了关键字yield，因而该类可做生成器(generater)。<br>在后续<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> train_dl:</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><br>的过程中会动态的将数据应用preprocess方法返回。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">x,y</span>):</span><br><span class="line">    <span class="keyword">return</span> x.to(dev),y.to(dev)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WrappedDataLoader</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dl, func</span>):</span><br><span class="line">        self.dl = dl</span><br><span class="line">        self.func = func</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.dl)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> b <span class="keyword">in</span> self.dl:</span><br><span class="line">            <span class="keyword">yield</span> (self.func(*b))</span><br><span class="line">train_dl = WrappedDataLoader(train_dl,preprocess)</span><br><span class="line">valid_dl = WrappedDataLoader(valid_dl,preprocess)</span><br></pre></td></tr></table></figure>
<p>将模型迁移到GPU</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.to(dev)</span><br><span class="line">opt = optim.SGD(model.parameters(), lr=learning_rate, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<p>试运行训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit(epochs, model, loss_func, opt, train_dl, valid_dl)</span><br></pre></td></tr></table></figure>
<p>检验准确率</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x,y = valid_ds[<span class="number">0</span>:batch_size]</span><br><span class="line">x = x.to(dev)</span><br><span class="line">y = y.to(dev)</span><br><span class="line">pred = model(x)</span><br><span class="line"><span class="built_in">print</span>(pred.dtype,y.dtype)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;loss_func = &#x27;</span>,loss_func(pred,y.long()))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;accuracy = &#x27;</span>,accuracy(pred,y))</span><br></pre></td></tr></table></figure>
<p>运行结果表明，固定训练轮次准确率较低，若以$1\times 10^{-9}$为损失函数的收敛阈值，则运行无法收敛，具体原因暂未得知。</p>
]]></content>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>info</title>
    <url>/2023/07/08/test/</url>
    <content><![CDATA[<p>有什么问题可以在评论区留言，或者email联系ianafp@zju.edu.cn</p>
]]></content>
      <tags>
        <tag>info</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2023/07/08/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
<video src="/vid.mp4">my video</video>



]]></content>
      <tags>
        <tag>博客使用说明</tag>
      </tags>
  </entry>
  <entry>
    <title>学习率调整方法</title>
    <url>/2023/07/17/%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>这篇文章整理了一些调整学习率的方法，并且根据<a href="https://ianafp.github.io/2023/07/15/resnet18%E5%9C%A8cifar10%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E8%AE%AD%E7%BB%83/">上一篇文章，在CIFAR10数据集上训练ResNet18</a>中的代码应用这些调整。</p>
<span id="more"></span>
<h2 id="StepLR"><a href="#StepLR" class="headerlink" title="StepLR"></a>StepLR</h2><p>使用torch.optim进行优化时，我们往往会使用step()接口更新参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">loss_batch</span>(<span class="params">model,loss_func,xb,yb,opt=<span class="literal">None</span></span>):</span><br><span class="line">    loss = loss_func(model(xb),yb.long())</span><br><span class="line">    <span class="keyword">if</span> opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line">    <span class="keyword">return</span> loss.item(),<span class="built_in">len</span>(xb)</span><br></pre></td></tr></table></figure>
<p>在上一篇文章中，每次对一个batch的数据训练然后更新参数。</p>
<p>在pytorch中，我们可以记录step的调用，设置等step更新学习率。</p>
<p><a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html">官网StepLR文档</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Assuming optimizer uses lr = 0.05 for all groups</span></span><br><span class="line"><span class="comment"># lr = 0.05     if epoch &lt; 30</span></span><br><span class="line"><span class="comment"># lr = 0.005    if 30 &lt;= epoch &lt; 60</span></span><br><span class="line"><span class="comment"># lr = 0.0005   if 60 &lt;= epoch &lt; 90</span></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line">scheduler = StepLR(optimizer, step_size=<span class="number">30</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    train(...)</span><br><span class="line">    validate(...)</span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure>
<p>在fit函数中加上权重更新</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">epochs,model,loss_func,opt,train_dl,valid_dl</span>):</span><br><span class="line">    epoch = <span class="number">0</span></span><br><span class="line">    pre_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">for</span> xb,yb <span class="keyword">in</span> train_dl:</span><br><span class="line">            loss_batch(model,loss_func,xb,yb.long(),opt)</span><br><span class="line">        scheduler.step()</span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            losses,nums = <span class="built_in">zip</span>(*[loss_batch(model,loss_func,xb,yb) <span class="keyword">for</span> xb,yb <span class="keyword">in</span> valid_dl])</span><br><span class="line">        val_loss = np.<span class="built_in">sum</span>(np.multiply(losses,nums)) / np.<span class="built_in">sum</span>(nums)</span><br><span class="line">        <span class="built_in">print</span>(epoch,val_loss)</span><br><span class="line">        <span class="keyword">import</span> math </span><br><span class="line">        <span class="keyword">if</span> math.fabs(val_loss-pre_loss)&lt;<span class="number">1e-7</span> <span class="keyword">or</span> epoch &gt; <span class="number">100</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        pre_loss = val_loss</span><br><span class="line">        epoch = epoch + <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h2 id="MULTISTEPLR"><a href="#MULTISTEPLR" class="headerlink" title="MULTISTEPLR"></a>MULTISTEPLR</h2><p><a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiStepLR.html">https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiStepLR.html</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Assuming optimizer uses lr = 0.05 for all groups</span></span><br><span class="line"><span class="comment"># lr = 0.05     if epoch &lt; 30</span></span><br><span class="line"><span class="comment"># lr = 0.005    if 30 &lt;= epoch &lt; 80</span></span><br><span class="line"><span class="comment"># lr = 0.0005   if epoch &gt;= 80</span></span><br><span class="line">scheduler = MultiStepLR(optimizer, milestones=[<span class="number">30</span>,<span class="number">80</span>], gamma=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    train(...)</span><br><span class="line">    validate(...)</span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure>
<p>multisteplr原理很简单，只要step的计数值达到milestones中的节点值，那么学习率就会进行一次变换。</p>
<p><a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html">https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html</a></p>
<h2 id="Expotianal-LR"><a href="#Expotianal-LR" class="headerlink" title="Expotianal LR"></a>Expotianal LR</h2><p><a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html">https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CLASStorch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=- <span class="number">1</span>, verbose=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>expotional LR 调整学习率以epoch为单位，每个epoch进行一次学习率的衰减。</p>
<h2 id="Linear-LR"><a href="#Linear-LR" class="headerlink" title="Linear LR"></a>Linear LR</h2><p><a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LinearLR.html">https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LinearLR.html</a>    </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Assuming optimizer uses lr = 0.05 for all groups</span></span><br><span class="line"><span class="comment"># lr = 0.025    if epoch == 0</span></span><br><span class="line"><span class="comment"># lr = 0.03125  if epoch == 1</span></span><br><span class="line"><span class="comment"># lr = 0.0375   if epoch == 2</span></span><br><span class="line"><span class="comment"># lr = 0.04375  if epoch == 3</span></span><br><span class="line"><span class="comment"># lr = 0.05    if epoch &gt;= 4</span></span><br><span class="line">scheduler = LinearLR(self.opt, start_factor=<span class="number">0.5</span>, total_iters=<span class="number">4</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    train(...)</span><br><span class="line">    validate(...)</span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure>
<p>linear lr是设置学习率的最大值和最小值，以epoch为单位线性变化。</p>
<h2 id="Cycle-LR"><a href="#Cycle-LR" class="headerlink" title="Cycle LR"></a>Cycle LR</h2><p><a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CyclicLR.html">https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CyclicLR.html</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CLASStorch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=<span class="number">2000</span>, step_size_down=<span class="literal">None</span>, mode=<span class="string">&#x27;triangular&#x27;</span>, gamma=<span class="number">1.0</span>, scale_fn=<span class="literal">None</span>, scale_mode=<span class="string">&#x27;cycle&#x27;</span>, cycle_momentum=<span class="literal">True</span>, base_momentum=<span class="number">0.8</span>, max_momentum=<span class="number">0.9</span>, last_epoch=- <span class="number">1</span>, verbose=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=<span class="number">0.01</span>, max_lr=<span class="number">0.1</span>)</span><br><span class="line">data_loader = torch.utils.data.DataLoader(...)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> data_loader:</span><br><span class="line">        train_batch(...)</span><br><span class="line">        scheduler.step()</span><br></pre></td></tr></table></figure>
<p>cycle lr 的工作模式是使得学习率处于循环的升降，以期望参数学习可以跳出鞍点。</p>
<h2 id="One-Cycle-Lr"><a href="#One-Cycle-Lr" class="headerlink" title="One Cycle Lr"></a>One Cycle Lr</h2><p><a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html?highlight=one+cycle+lr#torch.optim.lr_scheduler.OneCycleLR">https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html?highlight=one+cycle+lr#torch.optim.lr_scheduler.OneCycleLR</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_loader = torch.utils.data.DataLoader(...)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=<span class="number">0.01</span>, steps_per_epoch=<span class="built_in">len</span>(data_loader), epochs=<span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> data_loader:</span><br><span class="line">        train_batch(...)</span><br><span class="line">        scheduler.step()</span><br></pre></td></tr></table></figure>
<p>CycleLr的一周期版本</p>
<h2 id="Consine-Anealing-Lr"><a href="#Consine-Anealing-Lr" class="headerlink" title="Consine Anealing Lr"></a>Consine Anealing Lr</h2><p><a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html?highlight=cosineannealinglr#torch.optim.lr_scheduler.CosineAnnealingLR">https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html?highlight=cosineannealinglr#torch.optim.lr_scheduler.CosineAnnealingLR</a></p>
<p>余弦退火策略，使得学习率以余弦的规律变化，有利于逃脱鞍点。</p>
<h2 id="Cosine-Annealing-WarmRestarts"><a href="#Cosine-Annealing-WarmRestarts" class="headerlink" title="Cosine Annealing WarmRestarts"></a>Cosine Annealing WarmRestarts</h2><p><a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html?highlight=cosineannealingwarmrestarts#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts">https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html?highlight=cosineannealingwarmrestarts#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts</a></p>
<p>余弦退火策略的变种，在这个策略中，每轮迭代的轮数递增，且迭代中学习率处于余弦下降周期，完成一个周期后学习率突变到最大值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)</span><br><span class="line">iters = <span class="built_in">len</span>(dataloader)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    <span class="keyword">for</span> i, sample <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        inputs, labels = sample[<span class="string">&#x27;inputs&#x27;</span>], sample[<span class="string">&#x27;labels&#x27;</span>]</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        scheduler.step(epoch + i / iters)</span><br></pre></td></tr></table></figure>
<h2 id="Lambda-Lr"><a href="#Lambda-Lr" class="headerlink" title="Lambda Lr"></a>Lambda Lr</h2><p>自定义学习率</p>
<p><a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html?highlight=lambda#torch.optim.lr_scheduler.LambdaLR">https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html?highlight=lambda#torch.optim.lr_scheduler.LambdaLR</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CLASStorch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=- <span class="number">1</span>, verbose=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Assuming optimizer has two groups.</span></span><br><span class="line">lambda1 = <span class="keyword">lambda</span> epoch: epoch // <span class="number">30</span></span><br><span class="line">lambda2 = <span class="keyword">lambda</span> epoch: <span class="number">0.95</span> ** epoch</span><br><span class="line">scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    train(...)</span><br><span class="line">    validate(...)</span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure>
<p>lambda是以epoch(int)作为参数的匿名函数，返回epoch相关的学习率值。</p>
<h2 id="Sequencial-Lr"><a href="#Sequencial-Lr" class="headerlink" title="Sequencial Lr"></a>Sequencial Lr</h2><p><a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.SequentialLR.html?highlight=sequential#torch.optim.lr_scheduler.SequentialLR">https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.SequentialLR.html?highlight=sequential#torch.optim.lr_scheduler.SequentialLR</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Assuming optimizer uses lr = 1. for all groups</span></span><br><span class="line"><span class="comment"># lr = 0.1     if epoch == 0</span></span><br><span class="line"><span class="comment"># lr = 0.1     if epoch == 1</span></span><br><span class="line"><span class="comment"># lr = 0.9     if epoch == 2</span></span><br><span class="line"><span class="comment"># lr = 0.81    if epoch == 3</span></span><br><span class="line"><span class="comment"># lr = 0.729   if epoch == 4</span></span><br><span class="line">scheduler1 = ConstantLR(self.opt, factor=<span class="number">0.1</span>, total_iters=<span class="number">2</span>)</span><br><span class="line">scheduler2 = ExponentialLR(self.opt, gamma=<span class="number">0.9</span>)</span><br><span class="line">scheduler = SequentialLR(self.opt, schedulers=[scheduler1, scheduler2], milestones=[<span class="number">2</span>])</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    train(...)</span><br><span class="line">    validate(...)</span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CLASStorch.optim.lr_scheduler.SequentialLR(optimizer, schedulers, milestones, last_epoch=- <span class="number">1</span>, verbose=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>以milestone为界，顺序调用不同的scheduler。</p>
<h2 id="Chain-Scheduler"><a href="#Chain-Scheduler" class="headerlink" title="Chain Scheduler"></a>Chain Scheduler</h2><p><a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ChainedScheduler.html?highlight=chainedscheduler#torch.optim.lr_scheduler.ChainedScheduler">https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ChainedScheduler.html?highlight=chainedscheduler#torch.optim.lr_scheduler.ChainedScheduler</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Assuming optimizer uses lr = 1. for all groups</span></span><br><span class="line"><span class="comment"># lr = 0.09     if epoch == 0</span></span><br><span class="line"><span class="comment"># lr = 0.081    if epoch == 1</span></span><br><span class="line"><span class="comment"># lr = 0.729    if epoch == 2</span></span><br><span class="line"><span class="comment"># lr = 0.6561   if epoch == 3</span></span><br><span class="line"><span class="comment"># lr = 0.59049  if epoch &gt;= 4</span></span><br><span class="line">scheduler1 = ConstantLR(self.opt, factor=<span class="number">0.1</span>, total_iters=<span class="number">2</span>)</span><br><span class="line">scheduler2 = ExponentialLR(self.opt, gamma=<span class="number">0.9</span>)</span><br><span class="line">scheduler = ChainedScheduler([scheduler1, scheduler2])</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    train(...)</span><br><span class="line">    validate(...)</span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure>
<p>chainable scheduler 是和sequential scheduler同样，按照次序调用不同的scheduler，但是保证学习率连续。</p>
<h2 id="Constant-LR"><a href="#Constant-LR" class="headerlink" title="Constant LR"></a>Constant LR</h2><p><a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ConstantLR.html?highlight=constant">https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ConstantLR.html?highlight=constant</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Assuming optimizer uses lr = 0.05 for all groups</span></span><br><span class="line"><span class="comment"># lr = 0.025   if epoch == 0</span></span><br><span class="line"><span class="comment"># lr = 0.025   if epoch == 1</span></span><br><span class="line"><span class="comment"># lr = 0.025   if epoch == 2</span></span><br><span class="line"><span class="comment"># lr = 0.025   if epoch == 3</span></span><br><span class="line"><span class="comment"># lr = 0.05    if epoch &gt;= 4</span></span><br><span class="line">scheduler = ConstantLR(self.opt, factor=<span class="number">0.5</span>, total_iters=<span class="number">4</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    train(...)</span><br><span class="line">    validate(...)</span><br><span class="line">    scheduler.step()</span><br></pre></td></tr></table></figure>
<p>constant lr即把学习率设为常数，保持一定epoch</p>
<h2 id="Reduce-LR-On-Plateau"><a href="#Reduce-LR-On-Plateau" class="headerlink" title="Reduce LR On Plateau"></a>Reduce LR On Plateau</h2><p><a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html?highlight=reducelronplateau#torch.optim.lr_scheduler.ReduceLROnPlateau">https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html?highlight=reducelronplateau#torch.optim.lr_scheduler.ReduceLROnPlateau</a></p>
<p>Reduce Lr On Plateau以学习率或者损失函数为观测对象，在更新学习率时根据学习率和损失函数为性能是否提升的标准，自适应更新学习率。 </p>
]]></content>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>test-function</title>
    <url>/2023/07/11/test-function/</url>
    <content><![CDATA[<p>本篇用于测试markdown功能<br><span id="more"></span><br><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">iframe</span> <span class="attr">src</span> = <span class="string">&quot;./vid.mp4&quot;</span> &gt;</span>测试插入视频<span class="tag">&lt;/<span class="name">iframe</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">style</span>=<span class="string">&quot;position: relative; width: 100%; height: 0; padding-bottom: 75%;&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">iframe</span> </span></span><br><span class="line"><span class="tag"><span class="attr">src</span>=<span class="string">&quot;/vid.mp4&quot;</span> <span class="attr">scrolling</span>=<span class="string">&quot;no&quot;</span> <span class="attr">border</span>=<span class="string">&quot;0&quot;</span> </span></span><br><span class="line"><span class="tag"><span class="attr">frameborder</span>=<span class="string">&quot;no&quot;</span> <span class="attr">framespacing</span>=<span class="string">&quot;0&quot;</span> <span class="attr">allowfullscreen</span>=<span class="string">&quot;true&quot;</span> <span class="attr">style</span>=<span class="string">&quot;position: absolute; width: 100%; </span></span></span><br><span class="line"><span class="string"><span class="tag">height: 100%; left: 0; top: 0;&quot;</span>&gt;</span> <span class="tag">&lt;/<span class="name">iframe</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p><iframe src="./vid.mp4">测试插入视频</iframe></p>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;"><iframe src="/vid.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; 
height: 100%; left: 0; top: 0;"> </iframe></div>]]></content>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习入门笔记1</title>
    <url>/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/</url>
    <content><![CDATA[<hr>
<p>本文记录了本人阅读《深度学习入门：基于Python的理论与实现》（斋藤康毅）一书做的一些笔记。</p>
<span id="more"></span>
<h2 id="1-感知机"><a href="#1-感知机" class="headerlink" title="1 感知机"></a>1 感知机</h2><p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708131745739.png" alt></p>
<p>感知机接收多个输入信号，输出一个信号。这里所说的“信号”可以想 象成电流或河流那样具备“流动性”的东西。像电流流过导线，向前方输送 电子一样，感知机的信号也会形成流，向前方输送信息。但是，和实际的电 流不同的是，感知机的信号只有“流/不 流 ”（ 1/0）两种取值。在《深度学习入门：基于Python的理论与实现》中，0 对应“不传递信号”，1对应“传递信号”。</p>
<p>而$w_1,w_2$是信号的权重，感知机的工作模式可以用以下公式描述。</p>
<script type="math/tex; mode=display">
y = 
\begin{cases}
0, \ w_1x_1+w_2x_2 <= \theta \\
1, \ otherwise
\end{cases}</script><p>此处的$\theta$应理解为一个阈值。</p>
<p>上式也可写作</p>
<script type="math/tex; mode=display">
y = \begin{cases}0, b + w_1x_1+w_2x_2 <= 0 \\1, \ otherwise\end{cases}</script><p>其中$b$称为偏差, 反应感知机易于激活的程度。</p>
<p>也可以将感知机中的偏差$b$显示地表达出来</p>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708160702807.png" alt="image-20230708160702807"></p>
<p>将上式进一步简化后可表达为</p>
<script type="math/tex; mode=display">
y = h(b+w_1x_1+w_2x_2) \\
h(x)= \begin{cases}
0, x \le 0 \\
1, x > 1
\end{cases}</script><h2 id="2-神经网络"><a href="#2-神经网络" class="headerlink" title="2 神经网络"></a>2 神经网络</h2><h3 id="2-1-神经网络层"><a href="#2-1-神经网络层" class="headerlink" title="2.1 神经网络层"></a>2.1 神经网络层</h3><p>一个神经网络的例子，本书中将输入层，中间层，输出层依次作为0，1，2层</p>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708161048708.png" alt="image-20230708161048708"></p>
<h3 id="2-2-激活函数"><a href="#2-2-激活函数" class="headerlink" title="2.2 激活函数"></a>2.2 激活函数</h3><p>在感知机部分中我们引入过$h(x)$, 这类函数一般都是将输入信号总和转为一个输出信号。</p>
<p>在感知机中的工作模式如下</p>
<script type="math/tex; mode=display">
a = b + w_1x_1+w_2x_2  \\
y = h(a)</script><p>在感知机中显示得表达出$h(x)$的工作</p>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708161520990.png" alt="image-20230708161520990"></p>
<h4 id="2-2-1-sigmod函数"><a href="#2-2-1-sigmod函数" class="headerlink" title="2.2.1 sigmod函数"></a>2.2.1 sigmod函数</h4><script type="math/tex; mode=display">
h(x) = \frac{1}{1+e^{-x}}</script><p>sigmod函数是一种常见的激活函数。</p>
<h4 id="2-2-2-阶跃函数"><a href="#2-2-2-阶跃函数" class="headerlink" title="2.2.2 阶跃函数"></a>2.2.2 阶跃函数</h4><p>阶跃函数很简单，就是根据一个阈值将连续输入转为离散化的输出，和第一部分感知机中讲述的$h(x)$ 是一样的，类似于我们在学微积分时候的sign函数。</p>
<p>以下是阶跃函数和sigmod函数的图像对比。</p>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708162025130.png" alt="image-20230708162025130"></p>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708162036814.png" alt="image-20230708162036814"></p>
<h4 id="2-2-3-ReLU"><a href="#2-2-3-ReLU" class="headerlink" title="2.2.3 ReLU"></a>2.2.3 ReLU</h4><p>ReLU函数是近来使用较多的函数。</p>
<script type="math/tex; mode=display">
h(x) = \begin{cases}
x, x\ge 0 \\
0, x < 0
\end{cases}</script><p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708162234505.png" alt="image-20230708162234505"></p>
<h3 id="2-3-神经网络的内积"><a href="#2-3-神经网络的内积" class="headerlink" title="2.3 神经网络的内积"></a>2.3 神经网络的内积</h3><p>神经网络的运算基础是通过矩阵的乘法，示意图如下</p>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708163116056.png" alt="image-20230708163116056"></p>
<p>多层神经网络如下图</p>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230708163211477.png" alt="image-20230708163211477"></p>
<h2 id="3-神经网络的学习"><a href="#3-神经网络的学习" class="headerlink" title="3 神经网络的学习"></a>3 神经网络的学习</h2><p>前面介绍了神经网络的工作模式，不难看出，保证神经网络正确工作（完成识别，分类等任务）的关键在于权重和神经元的数量。</p>
<p>神经网络的关键点就在于如何从数据中学习，获得正确的参数。</p>
<h3 id="训练数据和测试数据"><a href="#训练数据和测试数据" class="headerlink" title="训练数据和测试数据"></a>训练数据和测试数据</h3><p>机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和 实验等。首先，使用训练数据进行学习，寻找最优的参数；然后，使用测试 数据评价训练得到的模型的实际能力。</p>
<p>为什么需要将数据分为训练数据和测 试数据呢？因为我们追求的是模型的泛化能力。为了正确评价模型的泛化能 力，就必须划分训练数据和测试数据。另外，训练数据也可以称为监督数据。</p>
<p>仅仅用一个数据集去学习和评价参数，是无法进行正确评价的。 这样会导致可以顺利地处理某个数据集，但无法处理其他数据集的情况。顺 便说一下，只对某个数据集过度拟合的状态称为过拟合（over fitting）。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>损失函数(loss function)描述的是神经网络性能的恶劣程度,一般使用均方误差和交叉熵。</p>
<h4 id="均方误差"><a href="#均方误差" class="headerlink" title="均方误差"></a>均方误差</h4><script type="math/tex; mode=display">
E = \frac{1}{2}\sum_k(y_k-t_k)^2</script><p>其中,$y_k,t_k$分别表示神经网络的输出，监督数据，神经网络的输出和监督数据的差异的均方体现为均方误差。</p>
<h4 id="交叉熵误差"><a href="#交叉熵误差" class="headerlink" title="交叉熵误差"></a>交叉熵误差</h4><script type="math/tex; mode=display">
E = - \sum _k t_k\log y_k</script><p>采用交叉熵策略的道理是一般监督数据$t_k$往往采用独热码(one hot)，因此在正确标签的位是1，其余为0。</p>
<p>因此交叉熵误差事实上是测试的是在监督数据输出为正确的情况下，神经网络输出的误差。</p>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710144238609.png" alt="image-20230710144238609"></p>
<p>根据自然对数图像，很自然地，神经网络输出值越接近1（即正确值），交叉熵误差就越小。</p>
<h3 id="mini-batch"><a href="#mini-batch" class="headerlink" title="mini-batch"></a>mini-batch</h3><p>使用mini-batch的理由很自然，神经网络的训练数据集往往规模巨大，如果以全部数据为对象计算损失函数是成本巨大的。</p>
<p>因此我们往往从数据集中选出一部分来作为全部数据的近似，称为mini-batch,即小批量。</p>
<h3 id="梯度法"><a href="#梯度法" class="headerlink" title="梯度法"></a>梯度法</h3><p>关于函数，导数，偏导数，数值微分，梯度的概念自行查阅微积分资料。</p>
<p>机器学习的主要任务就是在学习时寻找最优参数（权重和偏置）</p>
<p>我们可以将神经网络的损失函数$L$看作是关于权重的多元函数.</p>
<script type="math/tex; mode=display">
L = f(w_1,w_2,...,w_n)</script><p>一般而言，神经网络参数空间巨大，$L$ 往往无法得到解析的表示，而我们只需要得到$L$的极小值点就可以达到优化的目的，我们认为$L$ 关于参数空间是连续且可微。</p>
<p>那么我们可以利用数值微分的方法得到$L$关于某参数$w_k$ 的数值微分</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial w_k} = \frac{L(w_k+\delta)-L(w_k)}{\delta}</script><p>进而我们得到L的梯度</p>
<script type="math/tex; mode=display">
grad(L) = (\frac{\partial L}{\partial w_1},\frac{\partial L}{\partial w_2},\frac{\partial L}{\partial w_3},...,\frac{\partial L}{\partial w_n})</script><p>利用梯度下降进行优化的迭代步骤如下:</p>
<script type="math/tex; mode=display">
W = W-\eta \frac{\partial L}{\partial W}</script><p>其中$\eta$为学习率，即权重向梯度方向迭代的步长。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>总结神经网络的学习算法，步骤如下：</p>
<ol>
<li>mini-batch</li>
<li>计算梯度</li>
<li>更新参数</li>
<li>重复步骤1，2，3到阈值</li>
</ol>
<p>由于使用的数据是随机的mini-batch数据，因此上述方法又称为随机梯度下降法。</p>
<h2 id="误差反向传播法"><a href="#误差反向传播法" class="headerlink" title="误差反向传播法"></a>误差反向传播法</h2><h3 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h3><p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710152529536.png" alt="image-20230710152529536"></p>
<p>这是一个最简单的计算图，描述的是买苹果的过程中，在苹果数量，单价，消费税三者作用下如何得到最终开销。</p>
<p>使用计算图描述计算过程的好处一是能够进行局部计算，二是能够通过误差反向传播法求导数。</p>
<h3 id="利用误差反向传播求导"><a href="#利用误差反向传播求导" class="headerlink" title="利用误差反向传播求导"></a>利用误差反向传播求导</h3><p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710152716600.png" alt="image-20230710152716600"></p>
<p>上图是一个用误差反向传播求出导数的例子（苹果的单价，总价对最终价格影响的贡献）。</p>
<h4 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h4><p>利用计算图进行反向传播计算导数的过程是从右向左传递，与我们日常接触的计算恰恰相反。</p>
<p>传递导数的原理在于<strong>链式法则</strong>。</p>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710153004628.png" alt="image-20230710153004628"></p>
<p>假设存在$y=f(x)$的计算，反向传播如上图所示。</p>
<p>$E$信号是右侧传来的信号，乘以$f$节点的局部导数$\frac{\partial y}{\partial x}$ 后作为新的导数信号向左侧传递。</p>
<p>事实上可以看出反向传播的过程遵循链式法则。</p>
<p>即$z = f(t),t=f(x)$,那么$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial t} \frac{\partial t}{\partial x} $ </p>
<p>链式法则是复合函数求导的性质，具体查阅微积分教材。</p>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710154633453.png" alt="image-20230710154633453"></p>
<p>使用链式法则表示的计算图过程如上。</p>
<h4 id="加法节点的反向传播"><a href="#加法节点的反向传播" class="headerlink" title="加法节点的反向传播"></a>加法节点的反向传播</h4><p>以$z = x+y$为对象，那么$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} = 1$</p>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710154858929.png" alt="image-20230710154858929"></p>
<h4 id="乘法节点的反向传播"><a href="#乘法节点的反向传播" class="headerlink" title="乘法节点的反向传播"></a>乘法节点的反向传播</h4><p>以$z = xy$为对象，那么$\frac{\partial z}{\partial x} =y, \frac{\partial z}{\partial y} = x$</p>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710154935192.png" alt="image-20230710154935192"></p>
<h4 id="实例代码"><a href="#实例代码" class="headerlink" title="实例代码"></a>实例代码</h4><p>有反向传播机制的加法层和乘法层。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AddLayer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        out = x + y</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dout</span>):</span><br><span class="line">        dx = dout * <span class="number">1</span></span><br><span class="line">        dy = dout * <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> dx, dy</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MulLayer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.x = <span class="literal">None</span></span><br><span class="line">        self.y = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        self.x = x</span><br><span class="line">        self.y = y</span><br><span class="line">        out = x * y</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dout</span>):</span><br><span class="line">        dx = dout * self.y <span class="comment"># 翻转x和y</span></span><br><span class="line">        dy = dout * self.x</span><br><span class="line">        <span class="keyword">return</span> dx, dy</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="考虑激活层的情形"><a href="#考虑激活层的情形" class="headerlink" title="考虑激活层的情形"></a>考虑激活层的情形</h4><p>考虑ReLU</p>
<script type="math/tex; mode=display">
y=\begin{cases}
x, x>0 \\
0, x\le 0
\end{cases}</script><p>那么，</p>
<script type="math/tex; mode=display">
\frac{\partial y}{\partial x} = \begin{cases}
1, x>0 \\
0, x\le 0
\end{cases}</script><p>对于sigmoid,</p>
<script type="math/tex; mode=display">
y = \frac{1}{1+e^{-x}} \\
\frac{dy}{dx} = - (\frac{1}{1+e^{-x}})^2 \cdot (-e^{-x}) = \frac{e^{-x}}{(1+e^{-x})^2} = y(1-y)</script><p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710155735826.png" alt="image-20230710155735826"></p>
<h4 id="考虑Affine层的实现"><a href="#考虑Affine层的实现" class="headerlink" title="考虑Affine层的实现"></a>考虑Affine层的实现</h4><p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710162332961.png" alt="image-20230710162332961"></p>
<p>Affine层实际上就是考虑了矩阵运算的清醒，事实上对于矩阵运算的反向传播，有下式</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial X} = \frac{\partial L}{\partial Y} \cdot W^T \\
\frac{\partial L}{\partial W} = X^T \cdot \frac{\partial L}{\partial Y}</script><p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710162647105.png" alt="image-20230710162647105"></p>
<h4 id="使用误差反向传播法的学习"><a href="#使用误差反向传播法的学习" class="headerlink" title="使用误差反向传播法的学习"></a>使用误差反向传播法的学习</h4><p>两层神经网络层的实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TwoLayerNet</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size,</span></span><br><span class="line"><span class="params">                 weight_init_std=<span class="number">0.01</span></span>):</span><br><span class="line">        <span class="comment"># 初始化权重</span></span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        self.params[<span class="string">&#x27;W1&#x27;</span>] = weight_init_std * \</span><br><span class="line">                            np.random.randn(input_size, hidden_size)</span><br><span class="line">        self.params[<span class="string">&#x27;b1&#x27;</span>] = np.zeros(hidden_size)</span><br><span class="line">        self.params[<span class="string">&#x27;W2&#x27;</span>] = weight_init_std * \</span><br><span class="line">                            np.random.randn(hidden_size, output_size)</span><br><span class="line">        self.params[<span class="string">&#x27;b2&#x27;</span>] = np.zeros(output_size)</span><br><span class="line">        <span class="comment"># 生成层</span></span><br><span class="line">        self.layers = OrderedDict()</span><br><span class="line">        self.layers[<span class="string">&#x27;Affine1&#x27;</span>] = \</span><br><span class="line">            Affine(self.params[<span class="string">&#x27;W1&#x27;</span>], self.params[<span class="string">&#x27;b1&#x27;</span>])</span><br><span class="line">        self.layers[<span class="string">&#x27;Relu1&#x27;</span>] = Relu()</span><br><span class="line">        self.layers[<span class="string">&#x27;Affine2&#x27;</span>] = \</span><br><span class="line">            Affine(self.params[<span class="string">&#x27;W2&#x27;</span>], self.params[<span class="string">&#x27;b2&#x27;</span>])</span><br><span class="line">        self.lastLayer = SoftmaxWithLoss()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers.values():</span><br><span class="line">            x = layer.forward(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="comment"># x: 输入数据, t:监督数据</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        y = self.predict(x)</span><br><span class="line">        <span class="keyword">return</span> self.lastLayer.forward(y, t)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        y = self.predict(x)</span><br><span class="line">        y = np.argmax(y, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> t.ndim != <span class="number">1</span> : t = np.argmax(t, axis=<span class="number">1</span>)</span><br><span class="line">accuracy = np.<span class="built_in">sum</span>(y == t) / <span class="built_in">float</span>(x.shape[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">return</span> accuracy</span><br><span class="line">    <span class="comment"># x: 输入数据, t:监督数据</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">numerical_gradient</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        loss_W = <span class="keyword">lambda</span> W: self.loss(x, t)</span><br><span class="line">        grads = &#123;&#125;</span><br><span class="line">        grads[<span class="string">&#x27;W1&#x27;</span>] = numerical_gradient(loss_W, self.params[<span class="string">&#x27;W1&#x27;</span>])</span><br><span class="line">        grads[<span class="string">&#x27;b1&#x27;</span>] = numerical_gradient(loss_W, self.params[<span class="string">&#x27;b1&#x27;</span>])</span><br><span class="line">        grads[<span class="string">&#x27;W2&#x27;</span>] = numerical_gradient(loss_W, self.params[<span class="string">&#x27;W2&#x27;</span>])</span><br><span class="line">        grads[<span class="string">&#x27;b2&#x27;</span>] = numerical_gradient(loss_W, self.params[<span class="string">&#x27;b2&#x27;</span>])</span><br><span class="line">        <span class="keyword">return</span> grads</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        <span class="comment"># forward</span></span><br><span class="line">        self.loss(x, t)</span><br><span class="line">        <span class="comment"># backward</span></span><br><span class="line">        dout = <span class="number">1</span></span><br><span class="line">        dout = self.lastLayer.backward(dout)</span><br><span class="line">        layers = <span class="built_in">list</span>(self.layers.values())</span><br><span class="line">        layers.reverse()</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> layers:</span><br><span class="line">            dout = layer.backward(dout)</span><br><span class="line">        <span class="comment">#设定 </span></span><br><span class="line">        grads = &#123;&#125;</span><br><span class="line">        grads[<span class="string">&#x27;W1&#x27;</span>] = self.layers[<span class="string">&#x27;Affine1&#x27;</span>].dW</span><br><span class="line">        grads[<span class="string">&#x27;b1&#x27;</span>] = self.layers[<span class="string">&#x27;Affine1&#x27;</span>].db</span><br><span class="line">        grads[<span class="string">&#x27;W2&#x27;</span>] = self.layers[<span class="string">&#x27;Affine2&#x27;</span>].dW</span><br><span class="line">        grads[<span class="string">&#x27;b2&#x27;</span>] = self.layers[<span class="string">&#x27;Affine2&#x27;</span>].db</span><br><span class="line">        <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>
<p>将前文中利用梯度下降求梯度的方法替换为使用误差反向传播。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys, os</span><br><span class="line"> sys.path.append(os.pardir)</span><br><span class="line"> <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> <span class="keyword">from</span> dataset.mnist <span class="keyword">import</span> load_mnist</span><br><span class="line"> <span class="keyword">from</span> two_layer_net <span class="keyword">import</span> TwoLayerNet</span><br><span class="line"> <span class="comment"># 读入数据</span></span><br><span class="line">(x_train, t_train), (x_test, t_test) = \</span><br><span class="line">    load_mnist(normalize=<span class="literal">True</span>, one_hot_label=<span class="literal">True</span>)</span><br><span class="line"> network = TwoLayerNet(input_size=<span class="number">784</span>, hidden_size=<span class="number">50</span>, output_size=<span class="number">10</span>)</span><br><span class="line"> iters_num = <span class="number">10000</span></span><br><span class="line"> train_size = x_train.shape[<span class="number">0</span>]</span><br><span class="line"> batch_size = <span class="number">100</span></span><br><span class="line"> learning_rate = <span class="number">0.1</span></span><br><span class="line"> train_loss_list = []</span><br><span class="line"> train_acc_list = []</span><br><span class="line"> test_acc_list = []</span><br><span class="line"> iter_per_epoch = <span class="built_in">max</span>(train_size / batch_size, <span class="number">1</span>)</span><br><span class="line"> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iters_num):</span><br><span class="line">    batch_mask = np.random.choice(train_size, batch_size)</span><br><span class="line">    x_batch = x_train[batch_mask]</span><br><span class="line">    t_batch = t_train[batch_mask]</span><br><span class="line">    <span class="comment"># 通过误差反向传播法求梯度</span></span><br><span class="line">	grad = network.gradient(x_batch, t_batch)</span><br><span class="line">    <span class="comment"># 更新</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> (<span class="string">&#x27;W1&#x27;</span>, <span class="string">&#x27;b1&#x27;</span>, <span class="string">&#x27;W2&#x27;</span>, <span class="string">&#x27;b2&#x27;</span>):</span><br><span class="line">        network.params[key] -= learning_rate * grad[key]</span><br><span class="line">    loss = network.loss(x_batch, t_batch)</span><br><span class="line">    train_loss_list.append(loss)</span><br><span class="line">    <span class="keyword">if</span> i % iter_per_epoch == <span class="number">0</span>:</span><br><span class="line">        train_acc = network.accuracy(x_train, t_train)</span><br><span class="line">        test_acc = network.accuracy(x_test, t_test)</span><br><span class="line">        train_acc_list.append(train_acc)</span><br><span class="line">        test_acc_list.append(test_acc)</span><br><span class="line">        <span class="built_in">print</span>(train_acc, test_acc)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>卷积神经网络即CNN，(Convolutional Neural Network)</p>
<p>常常用于图像识别，语音识别等场合。</p>
<h3 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h3><p>CNN结构和之前的神经网络结构类似，但是出现了卷积层(convolutional)和池化层(pooling)。</p>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710163237306.png" alt="image-20230710163237306"></p>
<p>相邻神经元都有连接的结构称为全连接结构，上图在Affine层实现了全连接。</p>
<p>那么在CNN中增加了卷积层和池化层后则如下图所示。</p>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710163356084.png" alt="image-20230710163356084"></p>
<h3 id="卷积运算"><a href="#卷积运算" class="headerlink" title="卷积运算"></a>卷积运算</h3><p>全连接层存在的问题就是全连接层输入时将多维数据拉成一维数据，损失了输入的形状信息，而卷积层则是在保留输入形状信息的基础上进行运算的。</p>
<p>而卷积层可以保持形状不变。当输入数据是图像时，卷积层会以3维 数据的形式接收输入数据，并同样以3维数据的形式输出至下一层。因此， 在CNN中，可以（有可能）正确理解图像等具有形状的数据。 </p>
<p>另外，CNN中，有时将卷积层的输入输出数据称为特征图（feature  map）。其中，卷积层的输入数据称为输入特征图（input feature map）， 输 出 数据称为输出特征图（output feature map）。本书中将“输入输出数据”和“特 征图”作为含义相同的词使用。</p>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710165057328.png" alt="image-20230710165057328"></p>
<p>上图是一个卷积运算的例子。</p>
<p>卷积运算事实上是滤波器（也称卷积核）与输入数据各个位置上对应元素相乘再相加。</p>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710165211078.png" alt="image-20230710165211078"></p>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710165228579.png" alt="image-20230710165228579"></p>
<p>对于偏置，对所有元素进行偏置。</p>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710165253407.png" alt="image-20230710165253407"></p>
<h4 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h4><p>为了控制卷积生成的形状，我们常常要对输入数据先进行填充。</p>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710165406387.png" alt="image-20230710165406387"></p>
<h4 id="步幅"><a href="#步幅" class="headerlink" title="步幅"></a>步幅</h4><p>卷积核在输入数据上移动的步长称为步幅，步幅越大，卷积输出的规模越小。</p>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710165527994.png" alt="image-20230710165527994"></p>
<h4 id="多维数据的卷积运算"><a href="#多维数据的卷积运算" class="headerlink" title="多维数据的卷积运算"></a>多维数据的卷积运算</h4><p>这里以3通道的数据为例， 展示了卷积运算的结果。和2维数据时相比，可以发现纵深 方向（通道方向）上特征图增加了。通道方向上有多个特征图时，会按通道 进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出。</p>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710165718840.png" alt="image-20230710165718840"></p>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710165725537.png" alt="image-20230710165725537"></p>
<p>那么如何使得生成结果中具有更多维度呢？</p>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710190037365.png" alt="image-20230710190037365"></p>
<p>结合方块思考，我们可以将三维矩阵的卷积抽象为上述方块，立方体*立方体得到正方形，</p>
<p>如果想要得到立方体的结果，那么卷积核就要再提高一个维度，也就是将N个卷积核的二维结果拼接成三维。</p>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710190214544.png" alt="image-20230710190214544"></p>
<p>如果应用批处理，那么就在输入数据上再增加一个批维度。</p>
<p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710190353649.png" alt="image-20230710190353649"></p>
<p>像这样，数据作为4维的形状在各层间传递。这里需要注意的是，网络间传 递的是4维数据，对这N个数据进行了卷积运算。也就是说，批处理将N次 的处理汇总成了1次进行。</p>
<h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p><img src="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B01/image-20230710190517677.png" alt="image-20230710190517677"></p>
<p>池化运算是缩小矩阵规模的运算，上图中将2*2矩阵缩小为1*1,且应用Max池化，即选取矩阵中最大的元素作为池化结果。</p>
<p>除了Max池化之外，还有Average池化等。相对于Max池化是从 目标区域中取出最大值，Average池化则是计算目标区域的平均值。 在图像识别领域，主要使用Max池化。因此，本书中说到“池化层” 时，指的是Max池化。</p>
<p>池化层具有如下特征：</p>
<ul>
<li>没有要学习的参数</li>
<li>通道数不发生改变</li>
<li>对微小变化具有鲁班性，可以理解为应用max池化时，块中非max元素的改变对池化结果没有影响。</li>
</ul>
]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
</search>
